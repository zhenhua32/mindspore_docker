{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, GenerationConfig, TextStreamer\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM, LlamaTokenizerFast\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]c:\\tech\\Anaconda3\\envs\\nlp\\lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:31<00:00, 10.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "c:\\tech\\Anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "c:\\tech\\Anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_dir = r\"G:\\code\\pretrain_model_dir\\Llama-2-7b-hf\"\n",
    "# model_dir = r\"G:\\code\\pretrain_model_dir\\llama-7b-hf\"\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_dir)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_dir, \n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "print(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> write quick sort code by python:\n",
      "\n",
      "\\begin{code}\n",
      "def quick_sort(arr, low, high):\n",
      "   if low < high:\n",
      "       p = partition(arr, low, high)\n",
      "       quick_sort(arr, low, p - 1)\n",
      "       quick_sort(arr, p + 1, high)\n",
      "\n",
      "def partition(arr, low, high):\n",
      "   pivot = arr[high]\n",
      "   i = low - 1\n",
      "   for j in range(low, high):\n",
      "       if arr[j] <= pivot:\n",
      "           i += 1\n",
      "           arr[i], arr[j] = arr[j], arr[i]\n",
      "   arr[i + 1], arr[high] = arr[high], arr[i + 1]\n",
      "   return i + 1\n",
      "\\end{code}\n",
      "\n",
      "Comment: I'm not sure what you mean by \"write quick sort code by python\".\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\"write quick sort code by python:\", return_tensors=\"pt\").to(model.device)\n",
    "streamer = TextStreamer(tokenizer)\n",
    "\n",
    "# Despite returning the usual output, the streamer will also print the generated text to stdout.\n",
    "_ = model.generate(**inputs, streamer=streamer, max_new_tokens=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\tech\\Anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\tech\\Anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> write quick sort code by python:\n",
      "\n",
      "\\begin{code}\n",
      "def quick_sort(arr, low, high):\n",
      "    if low < high:\n",
      "        p = partition(arr, low, high)\n",
      "        quick_sort(arr, low, p - 1)\n",
      "        quick_sort(arr, p + 1, high)\n",
      "\n",
      "def partition(arr, low, high):\n",
      "    pivot = arr[high]\n",
      "    i = low - 1\n",
      "    for j in range(low, high):\n",
      "        if arr[j] <= pivot:\n",
      "            i += 1\n",
      "            arr[i], arr[j] = arr[j], arr[i]\n",
      "    arr[i + 1], arr[high] = arr[high], arr[i + 1]\n",
      "    return i + 1\n",
      "\\end{code}\n",
      "\n",
      "Comment: I'm not sure what you mean by \"write quick sort code by python\".\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\"write quick sort code by python:\", return_tensors=\"pt\").to(model.device)\n",
    "streamer = TextStreamer(tokenizer)\n",
    "\n",
    "# Despite returning the usual output, the streamer will also print the generated text to stdout.\n",
    "# _ = model.generate(**inputs, streamer=streamer, max_new_tokens=200)\n",
    "outputs = model.generate(**inputs, max_new_tokens=200)[0]\n",
    "result = tokenizer.decode(outputs, skip_special_tokens=False)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

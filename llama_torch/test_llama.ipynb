{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\tech\\Anaconda3\\envs\\nlp\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM, LlamaTokenizerFast\n",
    "from transformers.convert_slow_tokenizer import convert_slow_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = r\"G:\\code\\pretrain_model_dir\\open_llama_3b_v2\"\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.llama.tokenization_llama.LlamaTokenizer"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./data\\\\tokenizer_config.json',\n",
       " './data\\\\special_tokens_map.json',\n",
       " './data\\\\tokenizer.model',\n",
       " './data\\\\added_tokens.json')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"./data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>\n"
     ]
    }
   ],
   "source": [
    "tokenizer_fast = LlamaTokenizerFast.from_pretrained(model_path)\n",
    "print(type(tokenizer_fast))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer_fast.can_save_slow_tokenizer)\n",
    "print(tokenizer_fast.pretrained_vocab_files_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./data_fast\\\\tokenizer.model',)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_fast.save_vocabulary(\"./data_fast\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mg:\\code\\github\\mindspore_docker\\llama_torch\\test_llama.ipynb 单元格 8\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/code/github/mindspore_docker/llama_torch/test_llama.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# 如果不把 tokenizer.model 放到目录下, 使用 LlamaTokenizer 加载会报错, 但是 LlamaTokenizerFast 可以正常加载\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/code/github/mindspore_docker/llama_torch/test_llama.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model_path \u001b[39m=\u001b[39m \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mG:\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mcode\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mpretrain_model_dir\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mllama-7b\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/g%3A/code/github/mindspore_docker/llama_torch/test_llama.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m LlamaTokenizer\u001b[39m.\u001b[39;49mfrom_pretrained(model_path)\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/code/github/mindspore_docker/llama_torch/test_llama.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(tokenizer)\n",
      "File \u001b[1;32mc:\\tech\\Anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1854\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   1851\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1852\u001b[0m         logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mloading file \u001b[39m\u001b[39m{\u001b[39;00mfile_path\u001b[39m}\u001b[39;00m\u001b[39m from cache at \u001b[39m\u001b[39m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 1854\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_from_pretrained(\n\u001b[0;32m   1855\u001b[0m     resolved_vocab_files,\n\u001b[0;32m   1856\u001b[0m     pretrained_model_name_or_path,\n\u001b[0;32m   1857\u001b[0m     init_configuration,\n\u001b[0;32m   1858\u001b[0m     \u001b[39m*\u001b[39;49minit_inputs,\n\u001b[0;32m   1859\u001b[0m     token\u001b[39m=\u001b[39;49mtoken,\n\u001b[0;32m   1860\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[0;32m   1861\u001b[0m     local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[0;32m   1862\u001b[0m     _commit_hash\u001b[39m=\u001b[39;49mcommit_hash,\n\u001b[0;32m   1863\u001b[0m     _is_local\u001b[39m=\u001b[39;49mis_local,\n\u001b[0;32m   1864\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m   1865\u001b[0m )\n",
      "File \u001b[1;32mc:\\tech\\Anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2017\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[1;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2015\u001b[0m \u001b[39m# Instantiate tokenizer.\u001b[39;00m\n\u001b[0;32m   2016\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 2017\u001b[0m     tokenizer \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(\u001b[39m*\u001b[39;49minit_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minit_kwargs)\n\u001b[0;32m   2018\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[0;32m   2019\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\n\u001b[0;32m   2020\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUnable to load vocabulary from file. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2021\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease check that the provided vocabulary is accessible and not corrupted.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2022\u001b[0m     )\n",
      "File \u001b[1;32mc:\\tech\\Anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\models\\llama\\tokenization_llama.py:156\u001b[0m, in \u001b[0;36mLlamaTokenizer.__init__\u001b[1;34m(self, vocab_file, unk_token, bos_token, eos_token, pad_token, sp_model_kwargs, add_bos_token, add_eos_token, clean_up_tokenization_spaces, use_default_system_prompt, spaces_between_special_tokens, legacy, **kwargs)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_eos_token \u001b[39m=\u001b[39m add_eos_token\n\u001b[0;32m    154\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_default_system_prompt \u001b[39m=\u001b[39m use_default_system_prompt\n\u001b[1;32m--> 156\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msp_model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_spm_processor()\n\u001b[0;32m    157\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munk_token_length \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msp_model\u001b[39m.\u001b[39mencode(\u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39munk_token)))\n",
      "File \u001b[1;32mc:\\tech\\Anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\models\\llama\\tokenization_llama.py:162\u001b[0m, in \u001b[0;36mLlamaTokenizer.get_spm_processor\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_spm_processor\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    161\u001b[0m     tokenizer \u001b[39m=\u001b[39m spm\u001b[39m.\u001b[39mSentencePieceProcessor(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msp_model_kwargs)\n\u001b[1;32m--> 162\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvocab_file, \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m    163\u001b[0m         sp_model \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mread()\n\u001b[0;32m    164\u001b[0m         model_pb2 \u001b[39m=\u001b[39m import_protobuf()\n",
      "\u001b[1;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not NoneType"
     ]
    }
   ],
   "source": [
    "# 如果不把 tokenizer.model 放到目录下, 使用 LlamaTokenizer 加载会报错, 但是 LlamaTokenizerFast 可以正常加载\n",
    "model_path = r\"G:\\code\\pretrain_model_dir\\llama-7b\"\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_path)\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaTokenizerFast(name_or_path='G:\\code\\pretrain_model_dir\\llama-7b', vocab_size=32000, model_max_length=2048, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True)}, clean_up_tokenization_spaces=False)\n"
     ]
    }
   ],
   "source": [
    "model_path = r\"G:\\code\\pretrain_model_dir\\llama-7b\"\n",
    "tokenizer = LlamaTokenizerFast.from_pretrained(model_path)\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer._tokenizer.save(\"a.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in ./data_fast\\tokenizer_config.json\n",
      "Special tokens file saved in ./data_fast\\special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./data_fast\\\\tokenizer_config.json',\n",
       " './data_fast\\\\special_tokens_map.json',\n",
       " './data_fast\\\\tokenizer.json')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"./data_fast\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.can_save_slow_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vocab_file': {'hf-internal-testing/llama-tokenizer': 'https://huggingface.co/hf-internal-testing/llama-tokenizer/resolve/main/tokenizer.model'},\n",
       " 'tokenizer_file': {'hf-internal-testing/llama-tokenizer': 'https://huggingface.co/hf-internal-testing/llama-tokenizer/resolve/main/tokenizer_config.json'}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pretrained_vocab_files_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载模型\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    model_path, torch_dtype=torch.float16, device_map='auto',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>Q: What is the largest animal?\n",
      "A: The largest animal is the blue whale.\n",
      "Q: What is the smallest animal?\n",
      "A: The smallest animal is the dwarf chameleon\n"
     ]
    }
   ],
   "source": [
    "prompt = 'Q: What is the largest animal?\\nA:'\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "input_ids = input_ids.to(model.device)\n",
    "\n",
    "generation_output = model.generate(\n",
    "    input_ids=input_ids, max_new_tokens=32\n",
    ")\n",
    "print(tokenizer.decode(generation_output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>Q: can you explain what is quick sort?\n",
      "A: Quick sort is a sorting algorithm that is based on the divide and conquer principle. It is a recursive algorithm that divides the array into two sub-arrays, sorts the smaller sub-array, and then recursively sorts the larger sub-array.\n",
      "Q: What is the difference between quick sort and merge sort?\n",
      "A: Quick sort is a sorting algorithm that is based on the divide and conquer principle. It is a recursive algorithm that divides the array into two sub-arrays, sorts the smaller sub-array, and then recursively sorts the larger sub-array.\n",
      "Q: What is the difference between quick sort and merge sort?\n",
      "A: Quick sort is a sorting algorithm that is based on the divide and conquer principle. It is a recursive algorithm that divides the array into two sub-arrays, sorts the smaller sub-array, and then recursively sorts the larger sub-array.\n",
      "Q: What is the difference between quick sort and merge sort?\n",
      "A: Quick sort is a sorting algorithm that is based on the divide and conquer principle. It is a recursive algorithm that divides the array into two sub-arrays, sorts the smaller sub-array, and then recursively sorts the larger sub-array.\n",
      "Q: What is the difference between quick sort and merge sort?\n",
      "A: Quick sort is a sorting algorithm that is based on the divide and conquer principle. It is a recursive algorithm that divides the array into two sub-arrays, sorts the smaller sub-\n"
     ]
    }
   ],
   "source": [
    "prompt = 'Q: can you explain what is quick sort?\\nA:'\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "input_ids = input_ids.to(model.device)\n",
    "\n",
    "generation_output = model.generate(\n",
    "    input_ids=input_ids, max_new_tokens=320\n",
    ")\n",
    "print(tokenizer.decode(generation_output[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

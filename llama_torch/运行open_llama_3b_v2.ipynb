{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\tech\\Anaconda3\\envs\\nlp\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM, LlamaTokenizerFast\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=True`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>\n",
      "LlamaTokenizer(name_or_path='G:\\code\\pretrain_model_dir\\open_llama_3b_v2', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True)}, clean_up_tokenization_spaces=False)\n"
     ]
    }
   ],
   "source": [
    "model_path = r\"G:\\code\\pretrain_model_dir\\open_llama_3b_v2\"\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_path)\n",
    "print(type(tokenizer))\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bos_token <s> 1\n",
      "eos_token </s> 2\n",
      "pad_token None None\n",
      "add_bos_token True\n",
      "add_bos_token True\n"
     ]
    }
   ],
   "source": [
    "print(\"bos_token\", tokenizer.bos_token, tokenizer.bos_token_id)\n",
    "print(\"eos_token\", tokenizer.eos_token, tokenizer.eos_token_id)\n",
    "print(\"pad_token\", tokenizer.pad_token, tokenizer.pad_token_id)\n",
    "print(\"add_bos_token\", tokenizer.add_bos_token)\n",
    "print(\"add_bos_token\", tokenizer.add_bos_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float16 cuda:0\n"
     ]
    }
   ],
   "source": [
    "# 加载模型\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    model_path, torch_dtype=torch.float16, device_map='auto',\n",
    ")\n",
    "print(model.dtype, model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I look forward to the day when I can say that I have been a part of the world of blogging for a year. I have been blogging for a year now, and I have to say that I have enjoyed it']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 显存占用 10 GB\n",
    "prompt = \"I look forward to\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=40)\n",
    "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I love beijing , because it is a city of contrasts.\\nI love beijing , because it is a city of contrasts.\\nI love beijing , because it is a city of contrasts.\\nI']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"I love beijing , because\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=40)\n",
    "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerationConfig {\n",
       "  \"_from_model_config\": true,\n",
       "  \"bos_token_id\": 1,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"transformers_version\": \"4.32.1\"\n",
       "}"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "input_ids\n",
      "{'input_ids': tensor([[    1,   306,  1219,   339, 17336,  1518,   940]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n",
      "2048\n"
     ]
    }
   ],
   "source": [
    "print(model.config.is_encoder_decoder)\n",
    "print(model.main_input_name)\n",
    "print(inputs)\n",
    "print(model.config.max_position_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.use_return_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 3200, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (k_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (v_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (o_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=3200, out_features=8640, bias=False)\n",
       "          (up_proj): Linear(in_features=3200, out_features=8640, bias=False)\n",
       "          (down_proj): Linear(in_features=8640, out_features=3200, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (1): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (k_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (v_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (o_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=3200, out_features=8640, bias=False)\n",
       "          (up_proj): Linear(in_features=3200, out_features=8640, bias=False)\n",
       "          (down_proj): Linear(in_features=8640, out_features=3200, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (2): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (k_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (v_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (o_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=3200, out_features=8640, bias=False)\n",
       "          (up_proj): Linear(in_features=3200, out_features=8640, bias=False)\n",
       "          (down_proj): Linear(in_features=8640, out_features=3200, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (3): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (k_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (v_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (o_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=3200, out_features=8640, bias=False)\n",
       "          (up_proj): Linear(in_features=3200, out_features=8640, bias=False)\n",
       "          (down_proj): Linear(in_features=8640, out_features=3200, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (4): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (k_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (v_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (o_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=3200, out_features=8640, bias=False)\n",
       "          (up_proj): Linear(in_features=3200, out_features=8640, bias=False)\n",
       "          (down_proj): Linear(in_features=8640, out_features=3200, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (5): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (k_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (v_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (o_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=3200, out_features=8640, bias=False)\n",
       "          (up_proj): Linear(in_features=3200, out_features=8640, bias=False)\n",
       "          (down_proj): Linear(in_features=8640, out_features=3200, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (6): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (k_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (v_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (o_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=3200, out_features=8640, bias=False)\n",
       "          (up_proj): Linear(in_features=3200, out_features=8640, bias=False)\n",
       "          (down_proj): Linear(in_features=8640, out_features=3200, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (7): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (k_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (v_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (o_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=3200, out_features=8640, bias=False)\n",
       "          (up_proj): Linear(in_features=3200, out_features=8640, bias=False)\n",
       "          (down_proj): Linear(in_features=8640, out_features=3200, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (8): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (k_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (v_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (o_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=3200, out_features=8640, bias=False)\n",
       "          (up_proj): Linear(in_features=3200, out_features=8640, bias=False)\n",
       "          (down_proj): Linear(in_features=8640, out_features=3200, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (9): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (k_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (v_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (o_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=3200, out_features=8640, bias=False)\n",
       "          (up_proj): Linear(in_features=3200, out_features=8640, bias=False)\n",
       "          (down_proj): Linear(in_features=8640, out_features=3200, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (10): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (k_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (v_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (o_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=3200, out_features=8640, bias=False)\n",
       "          (up_proj): Linear(in_features=3200, out_features=8640, bias=False)\n",
       "          (down_proj): Linear(in_features=8640, out_features=3200, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (11): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (k_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (v_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (o_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=3200, out_features=8640, bias=False)\n",
       "          (up_proj): Linear(in_features=3200, out_features=8640, bias=False)\n",
       "          (down_proj): Linear(in_features=8640, out_features=3200, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (12): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (k_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (v_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (o_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=3200, out_features=8640, bias=False)\n",
       "          (up_proj): Linear(in_features=3200, out_features=8640, bias=False)\n",
       "          (down_proj): Linear(in_features=8640, out_features=3200, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (13): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (k_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (v_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (o_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=3200, out_features=8640, bias=False)\n",
       "          (up_proj): Linear(in_features=3200, out_features=8640, bias=False)\n",
       "          (down_proj): Linear(in_features=8640, out_features=3200, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (14): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (k_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (v_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (o_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=3200, out_features=8640, bias=False)\n",
       "          (up_proj): Linear(in_features=3200, out_features=8640, bias=False)\n",
       "          (down_proj): Linear(in_features=8640, out_features=3200, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (15): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (k_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (v_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (o_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=3200, out_features=8640, bias=False)\n",
       "          (up_proj): Linear(in_features=3200, out_features=8640, bias=False)\n",
       "          (down_proj): Linear(in_features=8640, out_features=3200, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (16): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (k_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (v_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (o_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=3200, out_features=8640, bias=False)\n",
       "          (up_proj): Linear(in_features=3200, out_features=8640, bias=False)\n",
       "          (down_proj): Linear(in_features=8640, out_features=3200, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (17): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (k_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (v_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (o_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=3200, out_features=8640, bias=False)\n",
       "          (up_proj): Linear(in_features=3200, out_features=8640, bias=False)\n",
       "          (down_proj): Linear(in_features=8640, out_features=3200, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (18): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (k_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (v_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (o_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=3200, out_features=8640, bias=False)\n",
       "          (up_proj): Linear(in_features=3200, out_features=8640, bias=False)\n",
       "          (down_proj): Linear(in_features=8640, out_features=3200, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (19): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (k_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (v_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (o_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=3200, out_features=8640, bias=False)\n",
       "          (up_proj): Linear(in_features=3200, out_features=8640, bias=False)\n",
       "          (down_proj): Linear(in_features=8640, out_features=3200, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (20): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (k_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (v_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (o_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=3200, out_features=8640, bias=False)\n",
       "          (up_proj): Linear(in_features=3200, out_features=8640, bias=False)\n",
       "          (down_proj): Linear(in_features=8640, out_features=3200, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (21): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (k_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (v_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (o_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=3200, out_features=8640, bias=False)\n",
       "          (up_proj): Linear(in_features=3200, out_features=8640, bias=False)\n",
       "          (down_proj): Linear(in_features=8640, out_features=3200, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (22): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (k_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (v_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (o_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=3200, out_features=8640, bias=False)\n",
       "          (up_proj): Linear(in_features=3200, out_features=8640, bias=False)\n",
       "          (down_proj): Linear(in_features=8640, out_features=3200, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (23): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (k_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (v_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (o_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=3200, out_features=8640, bias=False)\n",
       "          (up_proj): Linear(in_features=3200, out_features=8640, bias=False)\n",
       "          (down_proj): Linear(in_features=8640, out_features=3200, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (24): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (k_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (v_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (o_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=3200, out_features=8640, bias=False)\n",
       "          (up_proj): Linear(in_features=3200, out_features=8640, bias=False)\n",
       "          (down_proj): Linear(in_features=8640, out_features=3200, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (25): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (k_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (v_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (o_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=3200, out_features=8640, bias=False)\n",
       "          (up_proj): Linear(in_features=3200, out_features=8640, bias=False)\n",
       "          (down_proj): Linear(in_features=8640, out_features=3200, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3200, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 手动生成一个单词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    1,   306,  1219,   339, 17336,  1518,   940]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n",
      "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
      "torch.Size([1, 7])\n"
     ]
    }
   ],
   "source": [
    "prompt = \"I love beijing , because\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "print(inputs)\n",
    "print(type(inputs))\n",
    "print(inputs[\"input_ids\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "<s>\n",
      "</s>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.add_bos_token)\n",
    "print(tokenizer.add_eos_token)\n",
    "print(tokenizer.bos_token)\n",
    "print(tokenizer.eos_token)\n",
    "print(tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.modeling_outputs.CausalLMOutputWithPast'>\n",
      "odict_keys(['logits', 'past_key_values'])\n"
     ]
    }
   ],
   "source": [
    "result = model(**inputs)\n",
    "print(type(result))\n",
    "print(result.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 32000])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"logits\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-86.4375, -82.7500, -75.1875,  ..., -83.2500, -83.6875, -84.0625],\n",
       "         [-81.6250, -78.5000, -70.3750,  ..., -80.6250, -80.2500, -79.1250],\n",
       "         [-84.3125, -78.5000, -73.7500,  ..., -81.5625, -81.4375, -81.3750],\n",
       "         ...,\n",
       "         [-70.0625, -68.0625, -57.2500,  ..., -71.3750, -66.6250, -66.7500],\n",
       "         [-79.5625, -77.9375, -66.6875,  ..., -77.6875, -77.1250, -74.8750],\n",
       "         [-82.5625, -82.7500, -71.1875,  ..., -81.5625, -81.1250, -80.6875]]],\n",
       "       device='cuda:0', grad_fn=<ToCopyBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"logits\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([358], device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 这里只要选最后一个位置就行\n",
    "logits = result[\"logits\"][:, -1, :]\n",
    "torch.argmax(logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_one(prompt, verbose=False):\n",
    "    \"\"\"\n",
    "    手动生成一个单词, 并返回新的 prompt\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    if verbose:\n",
    "        print(inputs)\n",
    "        print(type(inputs))\n",
    "        print(inputs[\"input_ids\"].shape)\n",
    "\n",
    "    result = model(**inputs)\n",
    "    if verbose:\n",
    "        print(type(result))\n",
    "        print(result.keys())\n",
    "\n",
    "    # 这里只要选最后一个位置就行\n",
    "    logits = result[\"logits\"][:, -1, :]\n",
    "    token_id = torch.argmax(logits, dim=-1)\n",
    "    # 解码的时候怎么知道是否应加空格?\n",
    "    # 解码单个 token_id 的时候, 如果前缀是 ▁, 就表示一个单词的开始, 要加空格. 如果不是 ▁, 就是单词的中间或结尾, 不加空格.\n",
    "    new_input_ids = torch.cat([inputs[\"input_ids\"], token_id.unsqueeze(0)], dim=-1)\n",
    "    new_prompt = tokenizer.batch_decode(new_input_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "    return new_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▁contrast\n",
      "s\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(7352))\n",
    "print(tokenizer.convert_ids_to_tokens(29508))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9601, 95)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 是个特殊符号, 不是普通的下划线\n",
    "ord(\"▁\"), ord(\"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love beijing , because it\n",
      "I love beijing , because it is\n",
      "I love beijing , because it is a\n",
      "I love beijing , because it is a city\n",
      "I love beijing , because it is a city of\n",
      "I love beijing , because it is a city of contrast\n",
      "I love beijing , because it is a city of contrasts\n",
      "I love beijing , because it is a city of contrasts.\n",
      "I love beijing , because it is a city of contrasts.\n",
      "\n",
      "I love beijing , because it is a city of contrasts.\n",
      "I\n"
     ]
    }
   ],
   "source": [
    "# ['I love beijing , because it is a city of contrasts.\\nI love beijing , because it is a city of contrasts.\\nI love beijing , because it is a city of contrasts.\\nI']\n",
    "cur_prompt = prompt\n",
    "for i in range(10):\n",
    "    cur_prompt = generate_one(cur_prompt)\n",
    "    print(cur_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 调用 model.model 和 model.lm_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    1,   306,  1219,   339, 17336,  1518,   940]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n",
      "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
      "torch.Size([1, 7])\n"
     ]
    }
   ],
   "source": [
    "prompt = \"I love beijing , because\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "print(inputs)\n",
    "print(type(inputs))\n",
    "print(inputs[\"input_ids\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.modeling_outputs.BaseModelOutputWithPast'>\n",
      "odict_keys(['last_hidden_state', 'past_key_values'])\n"
     ]
    }
   ],
   "source": [
    "outputs = model.model(\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    "    # position_ids=position_ids,\n",
    "    # past_key_values=past_key_values,\n",
    "    # inputs_embeds=inputs_embeds,\n",
    "    # use_cache=use_cache,\n",
    "    # output_attentions=output_attentions,\n",
    "    # output_hidden_states=output_hidden_states,\n",
    "    # return_dict=return_dict,\n",
    ")\n",
    "print(type(outputs))\n",
    "print(outputs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 7, 3200])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.9629, -1.5957, -1.9365,  ...,  0.8457,  2.5410,  1.7910],\n",
       "         [-0.3489, -1.6348, -1.5059,  ...,  0.5723,  1.7080,  1.8477],\n",
       "         [ 1.4912, -1.6123, -1.8242,  ...,  1.5205,  3.3008,  2.6211],\n",
       "         ...,\n",
       "         [ 0.9775, -0.2898, -0.1992,  ..., -0.8467,  2.1504, -0.4502],\n",
       "         [ 1.5713, -1.5361, -1.5537,  ..., -1.0342,  0.4653,  0.3042],\n",
       "         [ 0.8340, -2.4160, -2.7832,  ...,  0.0782,  0.4045,  0.5225]]],\n",
       "       device='cuda:0', dtype=torch.float16, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_state = outputs[\"last_hidden_state\"]\n",
    "print(last_hidden_state.shape)\n",
    "last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-86.4375, -82.7500, -75.1875,  ..., -83.2500, -83.6875, -84.0625],\n",
       "         [-81.6250, -78.5000, -70.3750,  ..., -80.6250, -80.2500, -79.1250],\n",
       "         [-84.3125, -78.5000, -73.7500,  ..., -81.5625, -81.4375, -81.3750],\n",
       "         ...,\n",
       "         [-70.0625, -68.0625, -57.2500,  ..., -71.3750, -66.6250, -66.7500],\n",
       "         [-79.5625, -77.9375, -66.6875,  ..., -77.6875, -77.1250, -74.8750],\n",
       "         [-82.5625, -82.7500, -71.1875,  ..., -81.5625, -81.1250, -80.6875]]],\n",
       "       device='cuda:0', dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = model.lm_head(last_hidden_state)\n",
    "print(type(logits))\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([358], device='cuda:0')"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 这里只要选最后一个位置就行\n",
    "logits = logits[:, -1, :]\n",
    "torch.argmax(logits, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 每个解码层的输入和输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    1,   306,  1219,   339, 17336,  1518,   940]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n",
      "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
      "torch.Size([1, 7])\n"
     ]
    }
   ],
   "source": [
    "prompt = \"I love beijing , because\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "print(inputs)\n",
    "print(type(inputs))\n",
    "print(inputs[\"input_ids\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import  LlamaModel\n",
    "model: LlamaForCausalLM\n",
    "model.model: LlamaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 7, 3200])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.0315e-02,  2.8687e-03,  5.9128e-04,  ..., -1.8845e-03,\n",
       "          -9.2983e-05, -3.0060e-03],\n",
       "         [-1.0986e-02, -5.0735e-04,  3.3875e-03,  ...,  2.3346e-03,\n",
       "           9.3994e-03, -1.6602e-02],\n",
       "         [-1.6724e-02,  3.0212e-03, -1.4099e-02,  ...,  9.5749e-04,\n",
       "          -1.5137e-02,  2.4536e-02],\n",
       "         ...,\n",
       "         [ 2.5513e-02,  7.2021e-03,  8.4839e-03,  ...,  3.2654e-03,\n",
       "           7.2937e-03, -1.6602e-02],\n",
       "         [ 6.5002e-03, -6.9275e-03, -1.2451e-02,  ..., -1.9653e-02,\n",
       "           1.1658e-02,  2.1606e-02],\n",
       "         [-5.1880e-03, -1.3367e-02, -5.5695e-04,  ...,  3.3188e-04,\n",
       "           1.8066e-02, -8.9111e-03]]], device='cuda:0', dtype=torch.float16,\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 第一个是嵌入层\n",
    "inputs_embeds = model.model.embed_tokens(inputs[\"input_ids\"])\n",
    "# 3200 是指 hidden_size\n",
    "print(inputs_embeds.shape)\n",
    "inputs_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n",
      "<class 'transformers.models.llama.modeling_llama.LlamaDecoderLayer'>\n"
     ]
    }
   ],
   "source": [
    "# 这个模型有 26 层\n",
    "print(len(model.model.layers))\n",
    "print(type(model.model.layers[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 7, 7])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[     0., -65504., -65504., -65504., -65504., -65504., -65504.],\n",
       "          [     0.,      0., -65504., -65504., -65504., -65504., -65504.],\n",
       "          [     0.,      0.,      0., -65504., -65504., -65504., -65504.],\n",
       "          [     0.,      0.,      0.,      0., -65504., -65504., -65504.],\n",
       "          [     0.,      0.,      0.,      0.,      0., -65504., -65504.],\n",
       "          [     0.,      0.,      0.,      0.,      0.,      0., -65504.],\n",
       "          [     0.,      0.,      0.,      0.,      0.,      0.,      0.]]]],\n",
       "       device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 计算注意力\n",
    "batch_size, seq_length = inputs[\"input_ids\"].shape\n",
    "past_key_values_length = 0\n",
    "attention_mask = model.model._prepare_decoder_attention_mask(\n",
    "    # attention_mask shape 是 (batch_size, seq_length_with_past)\n",
    "    # 第二个参数是个 shape, (batch_size, seq_length)\n",
    "    # inputs_embeds 的 shape 是 (batch_size, seq_length, hidden_size)\n",
    "    inputs[\"attention_mask\"], (batch_size, seq_length), inputs_embeds, past_key_values_length\n",
    ")\n",
    "print(attention_mask.shape)\n",
    "attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 7])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2, 3, 4, 5, 6]], device='cuda:0')"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 自动构建 position_ids\n",
    "device = inputs[\"input_ids\"].device\n",
    "# shape 是 (seq_length)\n",
    "position_ids = torch.arange(\n",
    "    past_key_values_length, seq_length + past_key_values_length, dtype=torch.long, device=device\n",
    ")\n",
    "# shape 是 (1, seq_length). 不理解为啥是这样的shape\n",
    "position_ids = position_ids.unsqueeze(0).view(-1, seq_length)\n",
    "\n",
    "print(position_ids.shape)\n",
    "position_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 运行单层的结果\n",
    "hidden_states = inputs_embeds\n",
    "\n",
    "decoder_layer = model.model.layers[0]\n",
    "\n",
    "layer_outputs = decoder_layer(\n",
    "    # shape 是 (batch_size, seq_length, hidden_size)\n",
    "    hidden_states,\n",
    "    # shape 是 [bsz, 1, tgt_seq_len, src_seq_len]\n",
    "    attention_mask=attention_mask,\n",
    "    # shape 是 (batch_size, seq_length)\n",
    "    position_ids=position_ids,\n",
    "    # past_key_value=past_key_value,\n",
    "    # output_attentions=output_attentions,\n",
    "    # use_cache=use_cache,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 7, 3200])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0204, -0.0389,  0.0304,  ...,  0.0071,  0.0104,  0.0036],\n",
       "         [-0.0071, -0.0091,  0.0117,  ...,  0.0131,  0.0099, -0.0246],\n",
       "         [-0.0165,  0.0034, -0.0005,  ...,  0.0078, -0.0153,  0.0342],\n",
       "         ...,\n",
       "         [ 0.0217,  0.0018,  0.0030,  ...,  0.0015,  0.0135, -0.0089],\n",
       "         [ 0.0224, -0.0038, -0.0009,  ..., -0.0361,  0.0062,  0.0368],\n",
       "         [-0.0024, -0.0109,  0.0178,  ...,  0.0022,  0.0270, -0.0027]]],\n",
       "       device='cuda:0', dtype=torch.float16, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(layer_outputs))\n",
    "print(type(layer_outputs[0]))\n",
    "print(layer_outputs[0].shape)\n",
    "layer_outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.1504, -1.2461, -1.4385,  ...,  0.6284,  1.7852,  1.2754],\n",
       "         [-0.4331, -2.1855, -1.9160,  ...,  0.7275,  2.0547,  2.2520],\n",
       "         [ 1.9043, -2.2168, -2.3848,  ...,  1.9883,  4.0781,  3.2852],\n",
       "         ...,\n",
       "         [ 0.7256, -0.2314, -0.1514,  ..., -0.6431,  1.5449, -0.3279],\n",
       "         [ 2.1973, -2.3125, -2.2266,  ..., -1.4814,  0.6299,  0.4177],\n",
       "         [ 1.1953, -3.7266, -4.0859,  ...,  0.1150,  0.5615,  0.7354]]],\n",
       "       device='cuda:0', dtype=torch.float16, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 运行所有的层\n",
    "hidden_states = inputs_embeds\n",
    "for layer in model.model.layers:\n",
    "    layer_outputs = layer(\n",
    "        # shape 是 (batch_size, seq_length, hidden_size)\n",
    "        hidden_states,\n",
    "        # shape 是 [bsz, 1, tgt_seq_len, src_seq_len]\n",
    "        attention_mask=attention_mask,\n",
    "        # shape 是 (batch_size, seq_length)\n",
    "        position_ids=position_ids,\n",
    "        # past_key_value=past_key_value,\n",
    "        # output_attentions=output_attentions,\n",
    "        # use_cache=use_cache,\n",
    "    )\n",
    "\n",
    "    # 层的第一个输出是 hidden_states\n",
    "    hidden_states = layer_outputs[0]\n",
    "\n",
    "hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.9629, -1.5957, -1.9365,  ...,  0.8457,  2.5410,  1.7910],\n",
       "         [-0.3489, -1.6348, -1.5059,  ...,  0.5723,  1.7080,  1.8477],\n",
       "         [ 1.4912, -1.6123, -1.8242,  ...,  1.5205,  3.3008,  2.6211],\n",
       "         ...,\n",
       "         [ 0.9775, -0.2898, -0.1992,  ..., -0.8467,  2.1504, -0.4502],\n",
       "         [ 1.5713, -1.5361, -1.5537,  ..., -1.0342,  0.4653,  0.3042],\n",
       "         [ 0.8340, -2.4160, -2.7832,  ...,  0.0782,  0.4045,  0.5225]]],\n",
       "       device='cuda:0', dtype=torch.float16, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 经过 norm 层\n",
    "hidden_states = model.model.norm(hidden_states)\n",
    "hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 7, 32000])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-86.4375, -82.7500, -75.1875,  ..., -83.2500, -83.6875, -84.0625],\n",
       "         [-81.6250, -78.5000, -70.3750,  ..., -80.6250, -80.2500, -79.1250],\n",
       "         [-84.3125, -78.5000, -73.7500,  ..., -81.5625, -81.4375, -81.3750],\n",
       "         ...,\n",
       "         [-70.0625, -68.0625, -57.2500,  ..., -71.3750, -66.6250, -66.7500],\n",
       "         [-79.5625, -77.9375, -66.6875,  ..., -77.6875, -77.1250, -74.8750],\n",
       "         [-82.5625, -82.7500, -71.1875,  ..., -81.5625, -81.1250, -80.6875]]],\n",
       "       device='cuda:0', dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 经过 lm_head 层\n",
    "logits = model.lm_head(hidden_states)\n",
    "print(logits.shape)\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([358], device='cuda:0')"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = logits[:, -1, :]\n",
    "torch.argmax(logits, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 需要更深入一点, 查看解码层里面的每个模块的输入输出\n",
    "\n",
    "主要是 attention 里的forward 步骤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.llama.modeling_llama import *\n",
    "\n",
    "class LlamaAttention(nn.Module):\n",
    "    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\n",
    "    注意力层\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: LlamaConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        # 每个头的维度\n",
    "        self.head_dim = self.hidden_size // self.num_heads\n",
    "        # num_key_value_heads 默认是和 num_attention_heads (即 num_heads) 一样的\n",
    "        self.num_key_value_heads = config.num_key_value_heads\n",
    "        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n",
    "        # 最大位置嵌入\n",
    "        self.max_position_embeddings = config.max_position_embeddings\n",
    "        # rope_theta 默认是 10000.0, 即一万\n",
    "        self.rope_theta = config.rope_theta\n",
    "\n",
    "        if (self.head_dim * self.num_heads) != self.hidden_size:\n",
    "            raise ValueError(\n",
    "                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n",
    "                f\" and `num_heads`: {self.num_heads}).\"\n",
    "            )\n",
    "        # shape 是 (hidden_size, num_heads * head_dim), 也就是 (hidden_size, hidden_size)\n",
    "        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n",
    "        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n",
    "        # shape 是 (hidden_size, hidden_size)\n",
    "        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)\n",
    "        self._init_rope()\n",
    "\n",
    "    def _init_rope(self):\n",
    "        \"\"\"\n",
    "        初始化 RoPE, 也就是旋转位置编码\n",
    "        \"\"\"\n",
    "        if self.config.rope_scaling is None:\n",
    "            # 默认是空的, 应该用这个\n",
    "            self.rotary_emb = LlamaRotaryEmbedding(\n",
    "                self.head_dim,\n",
    "                max_position_embeddings=self.max_position_embeddings,\n",
    "                base=self.rope_theta,\n",
    "            )\n",
    "        else:\n",
    "            scaling_type = self.config.rope_scaling[\"type\"]\n",
    "            scaling_factor = self.config.rope_scaling[\"factor\"]\n",
    "            if scaling_type == \"linear\":\n",
    "                self.rotary_emb = LlamaLinearScalingRotaryEmbedding(\n",
    "                    self.head_dim,\n",
    "                    max_position_embeddings=self.max_position_embeddings,\n",
    "                    scaling_factor=scaling_factor,\n",
    "                    base=self.rope_theta,\n",
    "                )\n",
    "            elif scaling_type == \"dynamic\":\n",
    "                self.rotary_emb = LlamaDynamicNTKScalingRotaryEmbedding(\n",
    "                    self.head_dim,\n",
    "                    max_position_embeddings=self.max_position_embeddings,\n",
    "                    scaling_factor=scaling_factor,\n",
    "                    base=self.rope_theta,\n",
    "                )\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown RoPE scaling type {scaling_type}\")\n",
    "\n",
    "    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n",
    "        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
    "        output_attentions: bool = False,\n",
    "        use_cache: bool = False,\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
    "        \"\"\"\n",
    "        自注意力的前向传播\n",
    "        \"\"\"\n",
    "        # hidden_states 的 shape 是 (batch_size, seq_len, hidden_size)\n",
    "        bsz, q_len, _ = hidden_states.size()\n",
    "\n",
    "        if self.config.pretraining_tp > 1:\n",
    "            key_value_slicing = (self.num_key_value_heads * self.head_dim) // self.config.pretraining_tp\n",
    "            query_slices = self.q_proj.weight.split(\n",
    "                (self.num_heads * self.head_dim) // self.config.pretraining_tp, dim=0\n",
    "            )\n",
    "            key_slices = self.k_proj.weight.split(key_value_slicing, dim=0)\n",
    "            value_slices = self.v_proj.weight.split(key_value_slicing, dim=0)\n",
    "\n",
    "            query_states = [F.linear(hidden_states, query_slices[i]) for i in range(self.config.pretraining_tp)]\n",
    "            query_states = torch.cat(query_states, dim=-1)\n",
    "\n",
    "            key_states = [F.linear(hidden_states, key_slices[i]) for i in range(self.config.pretraining_tp)]\n",
    "            key_states = torch.cat(key_states, dim=-1)\n",
    "\n",
    "            value_states = [F.linear(hidden_states, value_slices[i]) for i in range(self.config.pretraining_tp)]\n",
    "            value_states = torch.cat(value_states, dim=-1)\n",
    "\n",
    "        else:\n",
    "            # 看这里, 更简单些, 上面是张量并行. 对 hidden_states 进行三种线性变换\n",
    "            # shape 是 (batch_size, seq_len, num_heads * head_dim)\n",
    "            query_states = self.q_proj(hidden_states)\n",
    "            # shape 是 (batch_size, seq_len, num_key_value_heads * head_dim)\n",
    "            key_states = self.k_proj(hidden_states)\n",
    "            value_states = self.v_proj(hidden_states)\n",
    "\n",
    "        # 现在 shape 是 (batch_size, num_heads, seq_len, head_dim)\n",
    "        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        # shape 是 (batch_size, num_key_value_heads, seq_len, head_dim)\n",
    "        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        kv_seq_len = key_states.shape[-2]\n",
    "        if past_key_value is not None:\n",
    "            # 如果有过去的 key_value\n",
    "            kv_seq_len += past_key_value[0].shape[-2]\n",
    "        # 这边知识获取 cos 和 sin\n",
    "        cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n",
    "        # 应用旋转位置编码\n",
    "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n",
    "\n",
    "        if past_key_value is not None:\n",
    "            # 就是在 seq_len 这个维度上加一, 另外就是注意这个是放在最前面的\n",
    "            # reuse k, v, self_attention\n",
    "            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n",
    "            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n",
    "\n",
    "        # 使用缓存, 就是保留这两个状态\n",
    "        past_key_value = (key_states, value_states) if use_cache else None\n",
    "\n",
    "        # repeat k/v heads if n_kv_heads < n_heads\n",
    "        key_states = repeat_kv(key_states, self.num_key_value_groups)\n",
    "        value_states = repeat_kv(value_states, self.num_key_value_groups)\n",
    "\n",
    "        # 这一步就是计算注意力权重, 就是 QK^T / sqrt(d_k)\n",
    "        # shape 是 (batch_size, num_heads, seq_len, seq_len)\n",
    "        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "\n",
    "        if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):\n",
    "            raise ValueError(\n",
    "                f\"Attention weights should be of size {(bsz, self.num_heads, q_len, kv_seq_len)}, but is\"\n",
    "                f\" {attn_weights.size()}\"\n",
    "            )\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n",
    "                raise ValueError(\n",
    "                    f\"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}\"\n",
    "                )\n",
    "            # 使用注意力掩码\n",
    "            attn_weights = attn_weights + attention_mask\n",
    "\n",
    "        # 应用 softmax\n",
    "        # upcast attention to fp32\n",
    "        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
    "        # 再和 V 的矩阵乘法\n",
    "        # shape 是 (batch_size, num_heads, seq_len, head_dim)\n",
    "        attn_output = torch.matmul(attn_weights, value_states)\n",
    "\n",
    "        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n",
    "            raise ValueError(\n",
    "                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n",
    "                f\" {attn_output.size()}\"\n",
    "            )\n",
    "\n",
    "        # shape 是 (batch_size, seq_len, num_heads, head_dim)\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "        # shape 是 (batch_size, seq_len, hidden_size)\n",
    "        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
    "\n",
    "        if self.config.pretraining_tp > 1:\n",
    "            attn_output = attn_output.split(self.hidden_size // self.config.pretraining_tp, dim=2)\n",
    "            o_proj_slices = self.o_proj.weight.split(self.hidden_size // self.config.pretraining_tp, dim=1)\n",
    "            attn_output = sum([F.linear(attn_output[i], o_proj_slices[i]) for i in range(self.config.pretraining_tp)])\n",
    "        else:\n",
    "            # 最后经过一个 o_proj 层\n",
    "            attn_output = self.o_proj(attn_output)\n",
    "\n",
    "        if not output_attentions:\n",
    "            attn_weights = None\n",
    "\n",
    "        # 固定返回三个字段\n",
    "        return attn_output, attn_weights, past_key_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "register_forward_hook() got an unexpected keyword argument 'with_kwargs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mg:\\code\\github\\mindspore_docker\\llama_torch\\运行open_llama_3b_v2.ipynb 单元格 43\u001b[0m line \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/code/github/mindspore_docker/llama_torch/%E8%BF%90%E8%A1%8Copen_llama_3b_v2.ipynb#Y100sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mfor\u001b[39;00m name, cur_module \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mnamed_modules():\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/code/github/mindspore_docker/llama_torch/%E8%BF%90%E8%A1%8Copen_llama_3b_v2.ipynb#Y100sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mlayers.0.self_attn\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/g%3A/code/github/mindspore_docker/llama_torch/%E8%BF%90%E8%A1%8Copen_llama_3b_v2.ipynb#Y100sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m         hook \u001b[39m=\u001b[39m cur_module\u001b[39m.\u001b[39;49mregister_forward_hook(hook_fn, with_kwargs\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/code/github/mindspore_docker/llama_torch/%E8%BF%90%E8%A1%8Copen_llama_3b_v2.ipynb#Y100sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/code/github/mindspore_docker/llama_torch/%E8%BF%90%E8%A1%8Copen_llama_3b_v2.ipynb#Y100sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mprint\u001b[39m(hook)\n",
      "\u001b[1;31mTypeError\u001b[0m: register_forward_hook() got an unexpected keyword argument 'with_kwargs'"
     ]
    }
   ],
   "source": [
    "# 定义一个 hook, 获取到 LlamaAttention 层的输入和输出\n",
    "\n",
    "hook_data = dict()\n",
    "def hook_fn(module, args, kwargs, model_output):\n",
    "    hook_data[\"module\"] = module\n",
    "    hook_data[\"args\"] = args\n",
    "    hook_data[\"kwargs\"] = kwargs\n",
    "    hook_data[\"output\"] = model_output\n",
    "    print(module)  # 打印子模块的名称\n",
    "    print(args)   # 打印输入张量\n",
    "    print(kwargs)   # 打印输入张量\n",
    "    print(model_output)  # 打印输出张量\n",
    "\n",
    "\n",
    "for name, cur_module in model.model.named_modules():\n",
    "    if name == \"layers.0.self_attn\":\n",
    "        hook = cur_module.register_forward_hook(hook_fn, with_kwargs=True)\n",
    "        break\n",
    "\n",
    "print(hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaAttention(\n",
      "  (q_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
      "  (k_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
      "  (v_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
      "  (o_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "()\n",
      "(tensor([[[ 6.8130e-03, -1.3229e-02,  1.5678e-03,  ..., -1.4114e-03,\n",
      "          -6.6233e-04,  4.9095e-03],\n",
      "         [ 7.0763e-03, -2.9011e-03,  3.7479e-03,  ...,  3.2997e-03,\n",
      "          -1.0672e-03,  2.5749e-03],\n",
      "         [ 4.9553e-03,  2.0504e-03,  8.1177e-03,  ...,  3.9482e-03,\n",
      "           7.9584e-04,  2.4757e-03],\n",
      "         ...,\n",
      "         [ 2.6150e-03, -3.2921e-03,  4.8676e-03,  ...,  1.3542e-04,\n",
      "           3.8223e-03,  4.8561e-03],\n",
      "         [ 8.7814e-03,  2.0962e-03,  1.1078e-02,  ...,  9.6178e-04,\n",
      "          -3.2959e-03,  2.8057e-03],\n",
      "         [ 6.3248e-03,  9.8133e-04,  9.1095e-03,  ...,  5.5046e-03,\n",
      "           9.9540e-06, -2.8396e-04]]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<UnsafeViewBackward0>), None, (tensor([[[[ 0.9951,  0.5054, -0.6982,  ..., -0.2886, -0.8604, -0.0536],\n",
      "          [ 0.3264,  0.0710, -0.0538,  ...,  1.1016,  0.7720, -0.2140],\n",
      "          [ 0.7554, -0.7666,  0.3066,  ..., -0.4060,  0.5107,  0.0732],\n",
      "          ...,\n",
      "          [-0.7119, -0.0929,  0.7554,  ..., -0.4014,  0.3584,  0.0932],\n",
      "          [-0.8691,  0.3359,  0.0392,  ..., -0.7383,  1.3154,  0.3860],\n",
      "          [ 0.0273,  0.6118, -0.1985,  ..., -0.6729,  0.5142,  0.2864]],\n",
      "\n",
      "         [[ 0.0042,  0.0674,  0.0069,  ..., -1.2861, -0.3591,  1.3955],\n",
      "          [ 1.2861, -0.0034,  0.1626,  ...,  0.3154,  0.3577, -0.3687],\n",
      "          [ 0.3137,  0.6831,  0.8662,  ...,  0.5820, -0.1292, -0.3208],\n",
      "          ...,\n",
      "          [-1.5332,  0.5166,  0.9043,  ...,  0.4546, -0.1969, -0.5820],\n",
      "          [-1.0449, -0.4944, -0.1826,  ...,  0.6421, -0.5918, -0.6338],\n",
      "          [ 0.5986, -0.8477, -0.6592,  ...,  0.3943, -0.6260, -0.4434]],\n",
      "\n",
      "         [[ 0.2196,  0.0660, -0.3069,  ..., -1.4961,  2.7207, -1.6699],\n",
      "          [ 0.8887,  0.6807, -0.4285,  ...,  2.4355, -2.2207,  1.0479],\n",
      "          [ 0.9893, -0.6323, -0.9004,  ..., -0.7827,  0.4419,  0.4285],\n",
      "          ...,\n",
      "          [-1.5205, -1.0381,  0.3381,  ..., -0.5381,  0.1726,  0.2949],\n",
      "          [-1.7744, -0.5562,  0.8286,  ..., -0.9565, -0.0532,  0.6055],\n",
      "          [-0.0432,  0.5566,  1.0059,  ..., -1.5049,  1.1660,  0.1642]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.3813,  0.1821, -0.3845,  ..., -0.9414, -2.7676, -0.7559],\n",
      "          [ 1.5176,  0.9180,  0.4382,  ..., -1.3311, -0.9375, -0.8032],\n",
      "          [ 0.0356,  0.1188,  0.6050,  ..., -0.2070, -0.0148, -0.6709],\n",
      "          ...,\n",
      "          [-0.9521, -0.6479,  0.4331,  ..., -0.1836, -0.1196, -0.2330],\n",
      "          [-0.6763, -0.5439, -0.1719,  ...,  0.0302, -0.3945, -0.3853],\n",
      "          [ 0.2703, -0.1609, -0.2798,  ..., -0.1127, -0.2725, -0.4888]],\n",
      "\n",
      "         [[-1.9824,  0.0773, -1.7373,  ...,  0.6147,  2.0547, -0.2686],\n",
      "          [ 1.2529, -1.8008, -0.2163,  ...,  0.5015, -0.2362, -0.4165],\n",
      "          [ 1.4561, -1.2686,  0.8213,  ...,  0.6626, -0.0875, -0.5859],\n",
      "          ...,\n",
      "          [-1.2822,  0.5078,  1.4414,  ...,  0.8311,  0.2150, -0.5259],\n",
      "          [-2.9570,  2.6797,  1.1758,  ...,  1.2568,  0.6509, -0.7871],\n",
      "          [-0.8994,  1.3086, -0.3193,  ...,  0.5083,  0.2998, -0.2300]],\n",
      "\n",
      "         [[-0.0674,  0.0156, -0.1633,  ...,  0.0795, -0.1406,  0.1118],\n",
      "          [ 1.8018, -1.0508, -0.5708,  ..., -1.6748, -0.4204, -0.8286],\n",
      "          [ 1.0039, -0.8062, -0.2993,  ...,  0.3677,  0.3259,  0.4246],\n",
      "          ...,\n",
      "          [-0.6567,  0.1160,  0.3750,  ...,  0.2539,  0.1334,  0.2448],\n",
      "          [-2.2461,  1.3184,  1.1533,  ..., -0.3623, -0.2778,  0.9033],\n",
      "          [-0.1149,  0.6455,  0.4404,  ...,  0.1064,  0.0703,  0.3184]]]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddBackward0>), tensor([[[[ 1.7334e-02,  3.0804e-03, -1.5900e-02,  ..., -3.8109e-03,\n",
      "            3.6645e-04, -4.8752e-03],\n",
      "          [-1.0475e-02,  8.4152e-03, -3.3417e-02,  ..., -1.5259e-02,\n",
      "           -8.4610e-03,  4.0016e-03],\n",
      "          [ 1.6388e-02,  2.3878e-04,  1.1871e-02,  ...,  1.7380e-02,\n",
      "           -1.6693e-02,  1.9440e-02],\n",
      "          ...,\n",
      "          [-6.5422e-03,  8.3313e-03,  1.2260e-02,  ...,  2.4063e-02,\n",
      "           -6.9904e-04,  5.3902e-03],\n",
      "          [ 1.8982e-02,  7.3814e-03,  5.2376e-03,  ...,  2.6108e-02,\n",
      "           -2.8191e-03, -1.1017e-02],\n",
      "          [-2.4700e-03,  1.5038e-02,  1.2566e-02,  ..., -2.2173e-05,\n",
      "           -4.6883e-03,  2.2095e-02]],\n",
      "\n",
      "         [[-1.8883e-03,  6.8665e-03, -2.1545e-02,  ...,  5.9586e-03,\n",
      "           -4.7150e-03, -6.0177e-04],\n",
      "          [-1.2840e-02,  2.2461e-02, -2.2240e-03,  ...,  7.2823e-03,\n",
      "           -1.2123e-02, -5.1079e-03],\n",
      "          [-1.2541e-03, -9.9106e-03, -1.0834e-02,  ...,  2.2793e-03,\n",
      "            1.1856e-02,  2.6245e-02],\n",
      "          ...,\n",
      "          [-4.0466e-02,  7.9041e-03,  2.1179e-02,  ...,  9.8495e-03,\n",
      "            2.1988e-02, -7.0992e-03],\n",
      "          [-9.4223e-04, -1.9569e-03,  1.9012e-02,  ..., -6.8741e-03,\n",
      "            2.1790e-02,  1.1307e-02],\n",
      "          [ 1.1269e-02, -2.1652e-02, -1.2047e-02,  ...,  1.1932e-02,\n",
      "            1.4023e-02,  8.2550e-03]],\n",
      "\n",
      "         [[ 4.4594e-03, -5.5885e-03,  1.4854e-02,  ..., -2.4776e-03,\n",
      "            7.9498e-03,  8.1329e-03],\n",
      "          [ 5.7173e-04,  1.2375e-02,  1.2915e-01,  ...,  4.9896e-03,\n",
      "           -2.1076e-03,  6.1707e-02],\n",
      "          [-1.2848e-02,  2.8973e-03,  2.6741e-03,  ..., -2.4292e-02,\n",
      "            6.0234e-03,  6.0692e-03],\n",
      "          ...,\n",
      "          [-4.6959e-03, -6.1607e-03, -2.9602e-02,  ..., -1.5335e-02,\n",
      "            1.3885e-03,  1.3222e-02],\n",
      "          [-7.2384e-04, -1.6159e-02,  5.5351e-03,  ..., -1.1959e-03,\n",
      "           -3.7937e-03, -2.6611e-02],\n",
      "          [ 1.9684e-02, -9.6512e-03,  9.6359e-03,  ...,  1.2833e-02,\n",
      "            4.4441e-03, -2.2858e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0327e-01,  1.3832e-02, -7.2861e-03,  ...,  5.1994e-03,\n",
      "           -5.0873e-02, -2.3575e-02],\n",
      "          [ 5.0449e-04,  3.7079e-02, -2.6016e-02,  ..., -7.2083e-02,\n",
      "            3.7781e-02, -1.2093e-02],\n",
      "          [-3.4851e-02, -3.6224e-02, -4.4342e-02,  ...,  1.4366e-02,\n",
      "           -4.0741e-02, -4.0741e-02],\n",
      "          ...,\n",
      "          [-7.4890e-02,  7.6599e-02, -2.1698e-02,  ...,  1.5747e-02,\n",
      "           -5.8228e-02, -4.2801e-03],\n",
      "          [ 2.6764e-02, -1.3901e-02, -2.1378e-02,  ...,  4.1168e-02,\n",
      "           -7.4341e-02, -5.3650e-02],\n",
      "          [ 5.6732e-02, -1.0620e-01,  1.8051e-02,  ...,  1.4832e-02,\n",
      "            6.0974e-02, -3.1860e-02]],\n",
      "\n",
      "         [[-3.5309e-02, -6.5735e-02,  3.4271e-02,  ...,  3.1662e-03,\n",
      "           -6.6711e-02,  1.8036e-02],\n",
      "          [-1.9638e-02, -1.8005e-02,  4.0222e-02,  ..., -6.5689e-03,\n",
      "           -5.3930e-04, -2.8419e-03],\n",
      "          [ 2.4673e-02,  2.3788e-02,  9.5215e-03,  ..., -3.0346e-03,\n",
      "           -1.7548e-02, -1.2703e-02],\n",
      "          ...,\n",
      "          [-1.8677e-02, -3.8818e-02, -1.3481e-02,  ..., -4.1565e-02,\n",
      "           -1.9287e-02,  4.7150e-02],\n",
      "          [ 5.1117e-03, -3.9886e-02, -1.3351e-02,  ...,  1.6571e-02,\n",
      "           -1.4133e-03, -3.7476e-02],\n",
      "          [ 8.6308e-04,  9.6436e-03,  8.1635e-03,  ..., -3.4607e-02,\n",
      "           -2.5925e-02, -7.5378e-03]],\n",
      "\n",
      "         [[ 1.8402e-02,  4.9744e-03,  5.9013e-03,  ...,  3.2501e-02,\n",
      "            3.6011e-03, -7.6981e-03],\n",
      "          [ 1.7044e-02,  6.0883e-03, -3.4332e-03,  ...,  1.0582e-02,\n",
      "            2.3346e-03,  5.5847e-03],\n",
      "          [ 9.2697e-03,  1.2245e-02, -1.3863e-02,  ..., -1.9272e-02,\n",
      "            2.8946e-02,  5.5313e-03],\n",
      "          ...,\n",
      "          [-1.2123e-02,  3.0777e-02,  4.3259e-03,  ...,  4.6790e-05,\n",
      "            1.7487e-02, -1.9592e-02],\n",
      "          [-2.9388e-02,  3.2997e-03, -1.5991e-02,  ...,  1.1620e-02,\n",
      "           -2.8992e-02,  4.4403e-03],\n",
      "          [ 3.0003e-03, -9.6588e-03,  1.0048e-02,  ..., -3.4580e-03,\n",
      "            6.7062e-03,  2.4939e-04]]]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<TransposeBackward0>)))\n",
      "LlamaAttention(\n",
      "  (q_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
      "  (k_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
      "  (v_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
      "  (o_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "()\n",
      "(tensor([[[ 6.8130e-03, -1.3229e-02,  1.5678e-03,  ..., -1.4114e-03,\n",
      "          -6.6233e-04,  4.9095e-03],\n",
      "         [ 7.0763e-03, -2.9011e-03,  3.7479e-03,  ...,  3.2997e-03,\n",
      "          -1.0672e-03,  2.5749e-03],\n",
      "         [ 4.9553e-03,  2.0504e-03,  8.1177e-03,  ...,  3.9482e-03,\n",
      "           7.9584e-04,  2.4757e-03],\n",
      "         ...,\n",
      "         [ 2.6150e-03, -3.2921e-03,  4.8676e-03,  ...,  1.3542e-04,\n",
      "           3.8223e-03,  4.8561e-03],\n",
      "         [ 8.7814e-03,  2.0962e-03,  1.1078e-02,  ...,  9.6178e-04,\n",
      "          -3.2959e-03,  2.8057e-03],\n",
      "         [ 6.3248e-03,  9.8133e-04,  9.1095e-03,  ...,  5.5046e-03,\n",
      "           9.9540e-06, -2.8396e-04]]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<UnsafeViewBackward0>), None, (tensor([[[[ 0.9951,  0.5054, -0.6982,  ..., -0.2886, -0.8604, -0.0536],\n",
      "          [ 0.3264,  0.0710, -0.0538,  ...,  1.1016,  0.7720, -0.2140],\n",
      "          [ 0.7554, -0.7666,  0.3066,  ..., -0.4060,  0.5107,  0.0732],\n",
      "          ...,\n",
      "          [-0.7119, -0.0929,  0.7554,  ..., -0.4014,  0.3584,  0.0932],\n",
      "          [-0.8691,  0.3359,  0.0392,  ..., -0.7383,  1.3154,  0.3860],\n",
      "          [ 0.0273,  0.6118, -0.1985,  ..., -0.6729,  0.5142,  0.2864]],\n",
      "\n",
      "         [[ 0.0042,  0.0674,  0.0069,  ..., -1.2861, -0.3591,  1.3955],\n",
      "          [ 1.2861, -0.0034,  0.1626,  ...,  0.3154,  0.3577, -0.3687],\n",
      "          [ 0.3137,  0.6831,  0.8662,  ...,  0.5820, -0.1292, -0.3208],\n",
      "          ...,\n",
      "          [-1.5332,  0.5166,  0.9043,  ...,  0.4546, -0.1969, -0.5820],\n",
      "          [-1.0449, -0.4944, -0.1826,  ...,  0.6421, -0.5918, -0.6338],\n",
      "          [ 0.5986, -0.8477, -0.6592,  ...,  0.3943, -0.6260, -0.4434]],\n",
      "\n",
      "         [[ 0.2196,  0.0660, -0.3069,  ..., -1.4961,  2.7207, -1.6699],\n",
      "          [ 0.8887,  0.6807, -0.4285,  ...,  2.4355, -2.2207,  1.0479],\n",
      "          [ 0.9893, -0.6323, -0.9004,  ..., -0.7827,  0.4419,  0.4285],\n",
      "          ...,\n",
      "          [-1.5205, -1.0381,  0.3381,  ..., -0.5381,  0.1726,  0.2949],\n",
      "          [-1.7744, -0.5562,  0.8286,  ..., -0.9565, -0.0532,  0.6055],\n",
      "          [-0.0432,  0.5566,  1.0059,  ..., -1.5049,  1.1660,  0.1642]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.3813,  0.1821, -0.3845,  ..., -0.9414, -2.7676, -0.7559],\n",
      "          [ 1.5176,  0.9180,  0.4382,  ..., -1.3311, -0.9375, -0.8032],\n",
      "          [ 0.0356,  0.1188,  0.6050,  ..., -0.2070, -0.0148, -0.6709],\n",
      "          ...,\n",
      "          [-0.9521, -0.6479,  0.4331,  ..., -0.1836, -0.1196, -0.2330],\n",
      "          [-0.6763, -0.5439, -0.1719,  ...,  0.0302, -0.3945, -0.3853],\n",
      "          [ 0.2703, -0.1609, -0.2798,  ..., -0.1127, -0.2725, -0.4888]],\n",
      "\n",
      "         [[-1.9824,  0.0773, -1.7373,  ...,  0.6147,  2.0547, -0.2686],\n",
      "          [ 1.2529, -1.8008, -0.2163,  ...,  0.5015, -0.2362, -0.4165],\n",
      "          [ 1.4561, -1.2686,  0.8213,  ...,  0.6626, -0.0875, -0.5859],\n",
      "          ...,\n",
      "          [-1.2822,  0.5078,  1.4414,  ...,  0.8311,  0.2150, -0.5259],\n",
      "          [-2.9570,  2.6797,  1.1758,  ...,  1.2568,  0.6509, -0.7871],\n",
      "          [-0.8994,  1.3086, -0.3193,  ...,  0.5083,  0.2998, -0.2300]],\n",
      "\n",
      "         [[-0.0674,  0.0156, -0.1633,  ...,  0.0795, -0.1406,  0.1118],\n",
      "          [ 1.8018, -1.0508, -0.5708,  ..., -1.6748, -0.4204, -0.8286],\n",
      "          [ 1.0039, -0.8062, -0.2993,  ...,  0.3677,  0.3259,  0.4246],\n",
      "          ...,\n",
      "          [-0.6567,  0.1160,  0.3750,  ...,  0.2539,  0.1334,  0.2448],\n",
      "          [-2.2461,  1.3184,  1.1533,  ..., -0.3623, -0.2778,  0.9033],\n",
      "          [-0.1149,  0.6455,  0.4404,  ...,  0.1064,  0.0703,  0.3184]]]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddBackward0>), tensor([[[[ 1.7334e-02,  3.0804e-03, -1.5900e-02,  ..., -3.8109e-03,\n",
      "            3.6645e-04, -4.8752e-03],\n",
      "          [-1.0475e-02,  8.4152e-03, -3.3417e-02,  ..., -1.5259e-02,\n",
      "           -8.4610e-03,  4.0016e-03],\n",
      "          [ 1.6388e-02,  2.3878e-04,  1.1871e-02,  ...,  1.7380e-02,\n",
      "           -1.6693e-02,  1.9440e-02],\n",
      "          ...,\n",
      "          [-6.5422e-03,  8.3313e-03,  1.2260e-02,  ...,  2.4063e-02,\n",
      "           -6.9904e-04,  5.3902e-03],\n",
      "          [ 1.8982e-02,  7.3814e-03,  5.2376e-03,  ...,  2.6108e-02,\n",
      "           -2.8191e-03, -1.1017e-02],\n",
      "          [-2.4700e-03,  1.5038e-02,  1.2566e-02,  ..., -2.2173e-05,\n",
      "           -4.6883e-03,  2.2095e-02]],\n",
      "\n",
      "         [[-1.8883e-03,  6.8665e-03, -2.1545e-02,  ...,  5.9586e-03,\n",
      "           -4.7150e-03, -6.0177e-04],\n",
      "          [-1.2840e-02,  2.2461e-02, -2.2240e-03,  ...,  7.2823e-03,\n",
      "           -1.2123e-02, -5.1079e-03],\n",
      "          [-1.2541e-03, -9.9106e-03, -1.0834e-02,  ...,  2.2793e-03,\n",
      "            1.1856e-02,  2.6245e-02],\n",
      "          ...,\n",
      "          [-4.0466e-02,  7.9041e-03,  2.1179e-02,  ...,  9.8495e-03,\n",
      "            2.1988e-02, -7.0992e-03],\n",
      "          [-9.4223e-04, -1.9569e-03,  1.9012e-02,  ..., -6.8741e-03,\n",
      "            2.1790e-02,  1.1307e-02],\n",
      "          [ 1.1269e-02, -2.1652e-02, -1.2047e-02,  ...,  1.1932e-02,\n",
      "            1.4023e-02,  8.2550e-03]],\n",
      "\n",
      "         [[ 4.4594e-03, -5.5885e-03,  1.4854e-02,  ..., -2.4776e-03,\n",
      "            7.9498e-03,  8.1329e-03],\n",
      "          [ 5.7173e-04,  1.2375e-02,  1.2915e-01,  ...,  4.9896e-03,\n",
      "           -2.1076e-03,  6.1707e-02],\n",
      "          [-1.2848e-02,  2.8973e-03,  2.6741e-03,  ..., -2.4292e-02,\n",
      "            6.0234e-03,  6.0692e-03],\n",
      "          ...,\n",
      "          [-4.6959e-03, -6.1607e-03, -2.9602e-02,  ..., -1.5335e-02,\n",
      "            1.3885e-03,  1.3222e-02],\n",
      "          [-7.2384e-04, -1.6159e-02,  5.5351e-03,  ..., -1.1959e-03,\n",
      "           -3.7937e-03, -2.6611e-02],\n",
      "          [ 1.9684e-02, -9.6512e-03,  9.6359e-03,  ...,  1.2833e-02,\n",
      "            4.4441e-03, -2.2858e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0327e-01,  1.3832e-02, -7.2861e-03,  ...,  5.1994e-03,\n",
      "           -5.0873e-02, -2.3575e-02],\n",
      "          [ 5.0449e-04,  3.7079e-02, -2.6016e-02,  ..., -7.2083e-02,\n",
      "            3.7781e-02, -1.2093e-02],\n",
      "          [-3.4851e-02, -3.6224e-02, -4.4342e-02,  ...,  1.4366e-02,\n",
      "           -4.0741e-02, -4.0741e-02],\n",
      "          ...,\n",
      "          [-7.4890e-02,  7.6599e-02, -2.1698e-02,  ...,  1.5747e-02,\n",
      "           -5.8228e-02, -4.2801e-03],\n",
      "          [ 2.6764e-02, -1.3901e-02, -2.1378e-02,  ...,  4.1168e-02,\n",
      "           -7.4341e-02, -5.3650e-02],\n",
      "          [ 5.6732e-02, -1.0620e-01,  1.8051e-02,  ...,  1.4832e-02,\n",
      "            6.0974e-02, -3.1860e-02]],\n",
      "\n",
      "         [[-3.5309e-02, -6.5735e-02,  3.4271e-02,  ...,  3.1662e-03,\n",
      "           -6.6711e-02,  1.8036e-02],\n",
      "          [-1.9638e-02, -1.8005e-02,  4.0222e-02,  ..., -6.5689e-03,\n",
      "           -5.3930e-04, -2.8419e-03],\n",
      "          [ 2.4673e-02,  2.3788e-02,  9.5215e-03,  ..., -3.0346e-03,\n",
      "           -1.7548e-02, -1.2703e-02],\n",
      "          ...,\n",
      "          [-1.8677e-02, -3.8818e-02, -1.3481e-02,  ..., -4.1565e-02,\n",
      "           -1.9287e-02,  4.7150e-02],\n",
      "          [ 5.1117e-03, -3.9886e-02, -1.3351e-02,  ...,  1.6571e-02,\n",
      "           -1.4133e-03, -3.7476e-02],\n",
      "          [ 8.6308e-04,  9.6436e-03,  8.1635e-03,  ..., -3.4607e-02,\n",
      "           -2.5925e-02, -7.5378e-03]],\n",
      "\n",
      "         [[ 1.8402e-02,  4.9744e-03,  5.9013e-03,  ...,  3.2501e-02,\n",
      "            3.6011e-03, -7.6981e-03],\n",
      "          [ 1.7044e-02,  6.0883e-03, -3.4332e-03,  ...,  1.0582e-02,\n",
      "            2.3346e-03,  5.5847e-03],\n",
      "          [ 9.2697e-03,  1.2245e-02, -1.3863e-02,  ..., -1.9272e-02,\n",
      "            2.8946e-02,  5.5313e-03],\n",
      "          ...,\n",
      "          [-1.2123e-02,  3.0777e-02,  4.3259e-03,  ...,  4.6790e-05,\n",
      "            1.7487e-02, -1.9592e-02],\n",
      "          [-2.9388e-02,  3.2997e-03, -1.5991e-02,  ...,  1.1620e-02,\n",
      "           -2.8992e-02,  4.4403e-03],\n",
      "          [ 3.0003e-03, -9.6588e-03,  1.0048e-02,  ..., -3.4580e-03,\n",
      "            6.7062e-03,  2.4939e-04]]]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<TransposeBackward0>)))\n",
      "LlamaAttention(\n",
      "  (q_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
      "  (k_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
      "  (v_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
      "  (o_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "()\n",
      "(tensor([[[ 6.8130e-03, -1.3229e-02,  1.5678e-03,  ..., -1.4114e-03,\n",
      "          -6.6233e-04,  4.9095e-03],\n",
      "         [ 7.0763e-03, -2.9011e-03,  3.7479e-03,  ...,  3.2997e-03,\n",
      "          -1.0672e-03,  2.5749e-03],\n",
      "         [ 4.9553e-03,  2.0504e-03,  8.1177e-03,  ...,  3.9482e-03,\n",
      "           7.9584e-04,  2.4757e-03],\n",
      "         ...,\n",
      "         [ 2.6150e-03, -3.2921e-03,  4.8676e-03,  ...,  1.3542e-04,\n",
      "           3.8223e-03,  4.8561e-03],\n",
      "         [ 8.7814e-03,  2.0962e-03,  1.1078e-02,  ...,  9.6178e-04,\n",
      "          -3.2959e-03,  2.8057e-03],\n",
      "         [ 6.3248e-03,  9.8133e-04,  9.1095e-03,  ...,  5.5046e-03,\n",
      "           9.9540e-06, -2.8396e-04]]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<UnsafeViewBackward0>), None, (tensor([[[[ 0.9951,  0.5054, -0.6982,  ..., -0.2886, -0.8604, -0.0536],\n",
      "          [ 0.3264,  0.0710, -0.0538,  ...,  1.1016,  0.7720, -0.2140],\n",
      "          [ 0.7554, -0.7666,  0.3066,  ..., -0.4060,  0.5107,  0.0732],\n",
      "          ...,\n",
      "          [-0.7119, -0.0929,  0.7554,  ..., -0.4014,  0.3584,  0.0932],\n",
      "          [-0.8691,  0.3359,  0.0392,  ..., -0.7383,  1.3154,  0.3860],\n",
      "          [ 0.0273,  0.6118, -0.1985,  ..., -0.6729,  0.5142,  0.2864]],\n",
      "\n",
      "         [[ 0.0042,  0.0674,  0.0069,  ..., -1.2861, -0.3591,  1.3955],\n",
      "          [ 1.2861, -0.0034,  0.1626,  ...,  0.3154,  0.3577, -0.3687],\n",
      "          [ 0.3137,  0.6831,  0.8662,  ...,  0.5820, -0.1292, -0.3208],\n",
      "          ...,\n",
      "          [-1.5332,  0.5166,  0.9043,  ...,  0.4546, -0.1969, -0.5820],\n",
      "          [-1.0449, -0.4944, -0.1826,  ...,  0.6421, -0.5918, -0.6338],\n",
      "          [ 0.5986, -0.8477, -0.6592,  ...,  0.3943, -0.6260, -0.4434]],\n",
      "\n",
      "         [[ 0.2196,  0.0660, -0.3069,  ..., -1.4961,  2.7207, -1.6699],\n",
      "          [ 0.8887,  0.6807, -0.4285,  ...,  2.4355, -2.2207,  1.0479],\n",
      "          [ 0.9893, -0.6323, -0.9004,  ..., -0.7827,  0.4419,  0.4285],\n",
      "          ...,\n",
      "          [-1.5205, -1.0381,  0.3381,  ..., -0.5381,  0.1726,  0.2949],\n",
      "          [-1.7744, -0.5562,  0.8286,  ..., -0.9565, -0.0532,  0.6055],\n",
      "          [-0.0432,  0.5566,  1.0059,  ..., -1.5049,  1.1660,  0.1642]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.3813,  0.1821, -0.3845,  ..., -0.9414, -2.7676, -0.7559],\n",
      "          [ 1.5176,  0.9180,  0.4382,  ..., -1.3311, -0.9375, -0.8032],\n",
      "          [ 0.0356,  0.1188,  0.6050,  ..., -0.2070, -0.0148, -0.6709],\n",
      "          ...,\n",
      "          [-0.9521, -0.6479,  0.4331,  ..., -0.1836, -0.1196, -0.2330],\n",
      "          [-0.6763, -0.5439, -0.1719,  ...,  0.0302, -0.3945, -0.3853],\n",
      "          [ 0.2703, -0.1609, -0.2798,  ..., -0.1127, -0.2725, -0.4888]],\n",
      "\n",
      "         [[-1.9824,  0.0773, -1.7373,  ...,  0.6147,  2.0547, -0.2686],\n",
      "          [ 1.2529, -1.8008, -0.2163,  ...,  0.5015, -0.2362, -0.4165],\n",
      "          [ 1.4561, -1.2686,  0.8213,  ...,  0.6626, -0.0875, -0.5859],\n",
      "          ...,\n",
      "          [-1.2822,  0.5078,  1.4414,  ...,  0.8311,  0.2150, -0.5259],\n",
      "          [-2.9570,  2.6797,  1.1758,  ...,  1.2568,  0.6509, -0.7871],\n",
      "          [-0.8994,  1.3086, -0.3193,  ...,  0.5083,  0.2998, -0.2300]],\n",
      "\n",
      "         [[-0.0674,  0.0156, -0.1633,  ...,  0.0795, -0.1406,  0.1118],\n",
      "          [ 1.8018, -1.0508, -0.5708,  ..., -1.6748, -0.4204, -0.8286],\n",
      "          [ 1.0039, -0.8062, -0.2993,  ...,  0.3677,  0.3259,  0.4246],\n",
      "          ...,\n",
      "          [-0.6567,  0.1160,  0.3750,  ...,  0.2539,  0.1334,  0.2448],\n",
      "          [-2.2461,  1.3184,  1.1533,  ..., -0.3623, -0.2778,  0.9033],\n",
      "          [-0.1149,  0.6455,  0.4404,  ...,  0.1064,  0.0703,  0.3184]]]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddBackward0>), tensor([[[[ 1.7334e-02,  3.0804e-03, -1.5900e-02,  ..., -3.8109e-03,\n",
      "            3.6645e-04, -4.8752e-03],\n",
      "          [-1.0475e-02,  8.4152e-03, -3.3417e-02,  ..., -1.5259e-02,\n",
      "           -8.4610e-03,  4.0016e-03],\n",
      "          [ 1.6388e-02,  2.3878e-04,  1.1871e-02,  ...,  1.7380e-02,\n",
      "           -1.6693e-02,  1.9440e-02],\n",
      "          ...,\n",
      "          [-6.5422e-03,  8.3313e-03,  1.2260e-02,  ...,  2.4063e-02,\n",
      "           -6.9904e-04,  5.3902e-03],\n",
      "          [ 1.8982e-02,  7.3814e-03,  5.2376e-03,  ...,  2.6108e-02,\n",
      "           -2.8191e-03, -1.1017e-02],\n",
      "          [-2.4700e-03,  1.5038e-02,  1.2566e-02,  ..., -2.2173e-05,\n",
      "           -4.6883e-03,  2.2095e-02]],\n",
      "\n",
      "         [[-1.8883e-03,  6.8665e-03, -2.1545e-02,  ...,  5.9586e-03,\n",
      "           -4.7150e-03, -6.0177e-04],\n",
      "          [-1.2840e-02,  2.2461e-02, -2.2240e-03,  ...,  7.2823e-03,\n",
      "           -1.2123e-02, -5.1079e-03],\n",
      "          [-1.2541e-03, -9.9106e-03, -1.0834e-02,  ...,  2.2793e-03,\n",
      "            1.1856e-02,  2.6245e-02],\n",
      "          ...,\n",
      "          [-4.0466e-02,  7.9041e-03,  2.1179e-02,  ...,  9.8495e-03,\n",
      "            2.1988e-02, -7.0992e-03],\n",
      "          [-9.4223e-04, -1.9569e-03,  1.9012e-02,  ..., -6.8741e-03,\n",
      "            2.1790e-02,  1.1307e-02],\n",
      "          [ 1.1269e-02, -2.1652e-02, -1.2047e-02,  ...,  1.1932e-02,\n",
      "            1.4023e-02,  8.2550e-03]],\n",
      "\n",
      "         [[ 4.4594e-03, -5.5885e-03,  1.4854e-02,  ..., -2.4776e-03,\n",
      "            7.9498e-03,  8.1329e-03],\n",
      "          [ 5.7173e-04,  1.2375e-02,  1.2915e-01,  ...,  4.9896e-03,\n",
      "           -2.1076e-03,  6.1707e-02],\n",
      "          [-1.2848e-02,  2.8973e-03,  2.6741e-03,  ..., -2.4292e-02,\n",
      "            6.0234e-03,  6.0692e-03],\n",
      "          ...,\n",
      "          [-4.6959e-03, -6.1607e-03, -2.9602e-02,  ..., -1.5335e-02,\n",
      "            1.3885e-03,  1.3222e-02],\n",
      "          [-7.2384e-04, -1.6159e-02,  5.5351e-03,  ..., -1.1959e-03,\n",
      "           -3.7937e-03, -2.6611e-02],\n",
      "          [ 1.9684e-02, -9.6512e-03,  9.6359e-03,  ...,  1.2833e-02,\n",
      "            4.4441e-03, -2.2858e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0327e-01,  1.3832e-02, -7.2861e-03,  ...,  5.1994e-03,\n",
      "           -5.0873e-02, -2.3575e-02],\n",
      "          [ 5.0449e-04,  3.7079e-02, -2.6016e-02,  ..., -7.2083e-02,\n",
      "            3.7781e-02, -1.2093e-02],\n",
      "          [-3.4851e-02, -3.6224e-02, -4.4342e-02,  ...,  1.4366e-02,\n",
      "           -4.0741e-02, -4.0741e-02],\n",
      "          ...,\n",
      "          [-7.4890e-02,  7.6599e-02, -2.1698e-02,  ...,  1.5747e-02,\n",
      "           -5.8228e-02, -4.2801e-03],\n",
      "          [ 2.6764e-02, -1.3901e-02, -2.1378e-02,  ...,  4.1168e-02,\n",
      "           -7.4341e-02, -5.3650e-02],\n",
      "          [ 5.6732e-02, -1.0620e-01,  1.8051e-02,  ...,  1.4832e-02,\n",
      "            6.0974e-02, -3.1860e-02]],\n",
      "\n",
      "         [[-3.5309e-02, -6.5735e-02,  3.4271e-02,  ...,  3.1662e-03,\n",
      "           -6.6711e-02,  1.8036e-02],\n",
      "          [-1.9638e-02, -1.8005e-02,  4.0222e-02,  ..., -6.5689e-03,\n",
      "           -5.3930e-04, -2.8419e-03],\n",
      "          [ 2.4673e-02,  2.3788e-02,  9.5215e-03,  ..., -3.0346e-03,\n",
      "           -1.7548e-02, -1.2703e-02],\n",
      "          ...,\n",
      "          [-1.8677e-02, -3.8818e-02, -1.3481e-02,  ..., -4.1565e-02,\n",
      "           -1.9287e-02,  4.7150e-02],\n",
      "          [ 5.1117e-03, -3.9886e-02, -1.3351e-02,  ...,  1.6571e-02,\n",
      "           -1.4133e-03, -3.7476e-02],\n",
      "          [ 8.6308e-04,  9.6436e-03,  8.1635e-03,  ..., -3.4607e-02,\n",
      "           -2.5925e-02, -7.5378e-03]],\n",
      "\n",
      "         [[ 1.8402e-02,  4.9744e-03,  5.9013e-03,  ...,  3.2501e-02,\n",
      "            3.6011e-03, -7.6981e-03],\n",
      "          [ 1.7044e-02,  6.0883e-03, -3.4332e-03,  ...,  1.0582e-02,\n",
      "            2.3346e-03,  5.5847e-03],\n",
      "          [ 9.2697e-03,  1.2245e-02, -1.3863e-02,  ..., -1.9272e-02,\n",
      "            2.8946e-02,  5.5313e-03],\n",
      "          ...,\n",
      "          [-1.2123e-02,  3.0777e-02,  4.3259e-03,  ...,  4.6790e-05,\n",
      "            1.7487e-02, -1.9592e-02],\n",
      "          [-2.9388e-02,  3.2997e-03, -1.5991e-02,  ...,  1.1620e-02,\n",
      "           -2.8992e-02,  4.4403e-03],\n",
      "          [ 3.0003e-03, -9.6588e-03,  1.0048e-02,  ..., -3.4580e-03,\n",
      "            6.7062e-03,  2.4939e-04]]]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<TransposeBackward0>)))\n"
     ]
    }
   ],
   "source": [
    "# 现在推理一把看看\n",
    "prompt = \"I love beijing , because\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "# print(inputs)\n",
    "# print(type(inputs))\n",
    "# print(inputs[\"input_ids\"].shape)\n",
    "\n",
    "result = model(**inputs)\n",
    "# print(type(result))\n",
    "# print(result.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

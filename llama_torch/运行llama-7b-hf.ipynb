{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\tech\\Anaconda3\\envs\\nlp\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM, LlamaTokenizerFast\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=True`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>\n",
      "LlamaTokenizer(name_or_path='G:\\code\\pretrain_model_dir\\llama-7b-hf', vocab_size=32000, model_max_length=1e+30, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"\", rstrip=False, lstrip=False, single_word=False, normalized=True)}, clean_up_tokenization_spaces=False)\n"
     ]
    }
   ],
   "source": [
    "model_path = r\"G:\\code\\pretrain_model_dir\\llama-7b-hf\"\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_path)\n",
    "print(type(tokenizer))\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# 这个分词器有点特殊, 没有定义这两个 token\n",
    "print(tokenizer.bos_token_id)\n",
    "print(tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用这个加载会有个 RecursionError, 不知道为啥\n",
    "# AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 33/33 [00:20<00:00,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float16 cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 加载模型\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    model_path, torch_dtype=torch.float16, device_map='auto',\n",
    ")\n",
    "print(model.dtype, model.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "windows 从来没有优先级, bitsandbytes 不支持 windows, 有一个别的版本的. 但好像 8 bit 更慢, 不知道是什么情况\n",
    "https://github.com/jllllll/bitsandbytes-windows-webui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin c:\\tech\\Anaconda3\\envs\\nlp\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda117.dll\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 33/33 [00:07<00:00,  4.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float16 cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 加载模型, 试试 8 bit 量化\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    load_in_8bit=True,\n",
    ")\n",
    "print(model.dtype, model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaConfig {\n",
       "  \"_name_or_path\": \"G:\\\\code\\\\pretrain_model_dir\\\\llama-7b-hf\",\n",
       "  \"architectures\": [\n",
       "    \"LLaMAForCausalLM\"\n",
       "  ],\n",
       "  \"bos_token_id\": 0,\n",
       "  \"eos_token_id\": 1,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 4096,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 11008,\n",
       "  \"max_position_embeddings\": 2048,\n",
       "  \"max_sequence_length\": 2048,\n",
       "  \"model_type\": \"llama\",\n",
       "  \"num_attention_heads\": 32,\n",
       "  \"num_hidden_layers\": 32,\n",
       "  \"num_key_value_heads\": 32,\n",
       "  \"pad_token_id\": -1,\n",
       "  \"pretraining_tp\": 1,\n",
       "  \"rms_norm_eps\": 1e-06,\n",
       "  \"rope_scaling\": null,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"torch_dtype\": \"float16\",\n",
       "  \"transformers_version\": \"4.32.1\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 32000\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerationConfig {\n",
       "  \"_from_model_config\": true,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"eos_token_id\": 1,\n",
       "  \"pad_token_id\": -1,\n",
       "  \"transformers_version\": \"4.32.1\"\n",
       "}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\tech\\Anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\generation\\utils.py:1411: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"I look forward to the next 10 years of the company.\\nI'm going to be a little bit more aggressive in the next 10 years. I'm going to be a little\"]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 这个推理速度很快, 比那个 3b-v2 的快多了. 显存占用 16 GB多\n",
    "prompt = \"I look forward to\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=40)\n",
    "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I love beijing , because I love beijing , I love beijing , because I love beijing , I love beijing , because I love beijing , I love beijing , because I']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"I love beijing , because\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=40)\n",
    "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试推理速度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: He opened his eyes and gaspe\n",
      ", time: 5.358856678009033\n",
      "['He opened his eyes and gaspe\\nHe was a man of sorrows and acquainted with grief\\nHe was despised and rejected of men\\nHe was wounded for our transgressions\\nHe was bruised for our in']\n",
      "query: She ran as fast as she coul\n",
      ", time: 5.417373418807983\n",
      "['She ran as fast as she coul\\nd. She ran as fast as she could\\ne. She ran as fast as she could\\nf. She ran as fast as she could\\ng. She ran as fast as she could\\n']\n",
      "query: The phone rang. He ignored i\n",
      ", time: 5.59221625328064\n",
      "['The phone rang. He ignored i\\nt. He was in the middle of a sentence. He was in the middle of a sentence. He was in the middle of a sentence. He was in the middle of a sentence. He was']\n",
      "query: They met at the airpor\n",
      ", time: 5.710055112838745\n",
      "['They met at the airpor\\nThe couple met at the airport in 2015, when they were both flying to New York.\\n\"I was on my way to New York and he was on his way to']\n",
      "query: She loved him. He didn’t kno\n",
      ", time: 5.3405821323394775\n",
      "['She loved him. He didn’t kno\\nWritten by: K.C. Wells\\nNarrated by: K.C. Wells\\nPublisher: K.C. Wells\\n©2016 K']\n",
      "query: He had a secret. A big on\n",
      ", time: 6.129676580429077\n",
      "['He had a secret. A big on\\ne. He was a spy.\\n10. The spy was a man. He was a man. He was a man. He was a man. He was a man. He']\n",
      "query: She hated her job. But she staye\n",
      ", time: 5.637867450714111\n",
      "['She hated her job. But she staye\\nd because she needed the money. She hated her boss. But she stayed because she needed the money. She hated her job. But she stayed because she needed the money. She h']\n",
      "query: The door slammed. He was gon\n",
      ", time: 5.783455848693848\n",
      "['The door slammed. He was gon\\nThe door slammed. He was gone.\\nI was alone in the house.\\nI was alone in the house. I was alone in the house. I was alone in the house. I']\n",
      "query: They found the treasure. And the tra\n",
      ", time: 5.9850640296936035\n",
      "['They found the treasure. And the tra\\ndition of the treasure is that the treasure is hidden in a place where the treasure is hidden. And the treasure is hidden in a place where the treasure is hidden. And']\n",
      "query: He was the last one aliv\n",
      ", time: 5.686009883880615\n",
      "['He was the last one aliv\\ne. He was the last one alive.\\nHe was the last one alive.\\nHe was the last one alive.\\nHe was the last one alive.\\nHe was the last one alive']\n",
      "query: She woke up in a strange place\n",
      ", time: 5.54996132850647\n",
      "['She woke up in a strange place\\nShe woke up in a strange place\\nShe woke up in a strange place.\\nShe woke up in a strange place.\\nShe woke up in a strange place.\\nShe']\n",
      "query: He had a plan. A brilliant one\n",
      ", time: 6.028300523757935\n",
      "['He had a plan. A brilliant one\\nI’m not sure I’m ready to be a mother. I’m not sure I’m ready to be a wife. I’m not sure I’m ready to be a']\n",
      "query: The letter changed everything\n",
      ", time: 5.902675628662109\n",
      "['The letter changed everything\\nThe letter changed everything\\nI was in the middle of a conversation with a friend when I got a text from my mom. It was a picture of a letter.\\nI was in the middle of']\n",
      "query: She saw him and smiled\n",
      ", time: 5.6314005851745605\n",
      "['She saw him and smiled\\nShe smiled and said, \"Hello, my love.\"\\nHe said, \"Hello, my love.\"\\nShe said, \"I\\'m so glad to see you.\"\\nHe said, \"']\n",
      "query: He was late. Again\n",
      ", time: 5.858022928237915\n",
      "['He was late. Again\\nHe was late. Again\\nHe was late. Again.\\nHe was late. Again.\\nHe was late. Again.\\nHe was late. Again.\\nHe was late. Again.']\n",
      "query: They were trapped. No escape\n",
      ", time: 5.888803482055664\n",
      "['They were trapped. No escape\\nTheir eyes were wide with fear.\\nTheir hearts were pounding.\\nThey were trapped.\\nNo escape.\\nTheir eyes were wide with fear.\\nTheir hearts were']\n",
      "query: She couldn’t believe her eyes\n",
      ", time: 5.732189655303955\n",
      "['She couldn’t believe her eyes\\nShe was so happy, she couldn’t believe her eyes\\nShe was so happy, she couldn’t believe her eyes.\\nShe was so happy, she couldn’t believe her eyes.']\n",
      "query: He heard a scream. He ran\n",
      ", time: 5.693226099014282\n",
      "['He heard a scream. He ran\\nto the room where the scream came from. He saw a man\\nholding a woman. The man was trying to rape the woman.\\nThe man was a rapist. The man']\n",
      "query: They kissed. Fireworks exploded\n",
      ", time: 5.7611305713653564\n",
      "['They kissed. Fireworks exploded\\nin the sky. The crowd cheered.\\nThe couple walked down the aisle.\\nThe bride wore a white dress.\\nThe groom wore a black suit.\\nThe b']\n",
      "query: She had a choice. A hard one\n",
      ", time: 5.930606365203857\n",
      "['She had a choice. A hard one\\nShe chose to be a mother.\\nShe chose to be a mother. She chose to be a mother. She chose to be a mother. She chose to be a mother. She chose to be']\n",
      "query: He had always wanted to fly\n",
      ", time: 5.7625744342803955\n",
      "['He had always wanted to fly\\nand now he was going to do it.\\nHe was going to fly in the sky.\\nHe was going to fly in the sky.\\nHe was going to fly in the sky.\\n']\n",
      "query: She was the best detective in town\n",
      ", time: 5.5347888469696045\n",
      "['She was the best detective in town\\nShe was the best detective in town\\nShe was the best detective in town.\\nShe was the best detective in town.\\nShe was the best detective in town.\\nShe']\n",
      "query: The war was over. But not for him\n",
      ", time: 6.101096153259277\n",
      "['The war was over. But not for him\\nThe war was over. But not for him\\nThe war was over. But not for him.\\nHe was a soldier, a man of war.\\nHe had fought for his country,\\n']\n",
      "query: She had a gift. A dangerous one\n",
      ", time: 6.047877788543701\n",
      "['She had a gift. A dangerous one\\nShe had a gift. A dangerous one\\nShe had a gift. A dangerous one.\\nShe had a gift. A dangerous one.\\nShe had a gift. A dangerous one.\\nShe']\n",
      "query: He didn’t expect to find love\n",
      ", time: 5.923654794692993\n",
      "['He didn’t expect to find love\\nI’m not sure if I’m going to read the next book in the series, but I’m glad I read this one.\\nI received a copy of this book from the publish']\n",
      "query: She was lost in the woods\n",
      ", time: 5.9323906898498535\n",
      "['She was lost in the woods\\nShe was lost in the woods\\nShe was lost in the woods.\\nShe was lost in the woods.\\nShe was lost in the woods.\\nShe was lost in the woods.\\nShe']\n",
      "query: The virus was spreading. Fast\n",
      ", time: 6.092569589614868\n",
      "['The virus was spreading. Fast\\nThe virus was spreading. Fast.\\nThe virus was spreading. Fast.\\nThe virus was spreading. Fast.\\nThe virus was spreading. Fast.\\nThe virus was spread']\n",
      "query: He had a mission. A secret one\n",
      ", time: 5.864466190338135\n",
      "['He had a mission. A secret one\\nthat would change the world.\\nHe was the Son of God.\\nThe Son of God was born in a stable.\\nThe Son of God was born in a stable. He was born in']\n",
      "query: She was a princess. But not by choice\n",
      ", time: 5.974654197692871\n",
      "['She was a princess. But not by choice\\nShe was a princess. But not by choice\\nShe was a princess. But not by choice.\\nShe was a princess. But not by choice. She was a princess.']\n",
      "query: He was a thief. A master one\n",
      ", time: 5.962935209274292\n",
      "['He was a thief. A master one\\nat that. He was a murderer. A master one at that. He was a\\nliar. A master one at that. He was a cheat. A master one at that.']\n",
      "[5.358856678009033, 5.417373418807983, 5.59221625328064, 5.710055112838745, 5.3405821323394775, 6.129676580429077, 5.637867450714111, 5.783455848693848, 5.9850640296936035, 5.686009883880615, 5.54996132850647, 6.028300523757935, 5.902675628662109, 5.6314005851745605, 5.858022928237915, 5.888803482055664, 5.732189655303955, 5.693226099014282, 5.7611305713653564, 5.930606365203857, 5.7625744342803955, 5.5347888469696045, 6.101096153259277, 6.047877788543701, 5.923654794692993, 5.9323906898498535, 6.092569589614868, 5.864466190338135, 5.974654197692871, 5.962935209274292]\n",
      "[40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40]\n",
      "每秒 token 数: 6.903912626163621\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# 记录每次生成的时间和 token 数量\n",
    "time_list = []\n",
    "token_list = []\n",
    "\n",
    "query_list = [\n",
    "    \"I look forward to\",\n",
    "    \"I love beijing , because\",\n",
    "]\n",
    "with open(\"./data/query.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    query_list = f.readlines()\n",
    "\n",
    "for query in query_list:\n",
    "    inputs = tokenizer(query, return_tensors=\"pt\").to(model.device)\n",
    "    start = time.time()\n",
    "    outputs = model.generate(**inputs, max_new_tokens=40)\n",
    "    end = time.time()\n",
    "    print(f\"query: {query}, time: {end - start}\")\n",
    "    print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
    "\n",
    "    time_list.append(end - start)\n",
    "    token_list.append(outputs.shape[1] - inputs.input_ids.shape[1])\n",
    "\n",
    "print(time_list)\n",
    "print(token_list)\n",
    "# 计算每秒生成的 token 数量\n",
    "print(\"每秒 token 数:\", sum(token_list) / sum(time_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

[TOC]

# 目的

将 `tokenizer.json` 转换成 sentencepiece 格式的 `tokenizer.model` 文件.

https://github.com/huggingface/transformers/issues/12728

[怎么让英文大预言模型支持中文？（一）构建自己的tokenization](https://www.cnblogs.com/xiximayou/p/17500806.html)

没想到可以用 `BloomTokenizer`, 底层分词算法都是 BPE, 而这个是接受 `tokenizer.json` 文件的, 所以可以直接用.

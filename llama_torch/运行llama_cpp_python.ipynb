{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 需要先转换下模型\n",
    "\n",
    "```bash\n",
    "python .\\convert.py G:\\code\\pretrain_model_dir\\llama-7b\n",
    "```\n",
    "\n",
    "TODO: 感觉好像没跑在 GPU 上, 需要看下文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "model_path = r\"G:\\code\\pretrain_model_dir\\llama-7b\\ggml-model-f16.gguf\"\n",
    "llm = Llama(model_path=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'cmpl-89dfcf44-6f62-4e4c-9553-c06e3b8e664c',\n",
       " 'object': 'text_completion',\n",
       " 'created': 1697119467,\n",
       " 'model': 'G:\\\\code\\\\pretrain_model_dir\\\\llama-7b\\\\ggml-model-f16.gguf',\n",
       " 'choices': [{'text': 'I look forward to sharing with you my work, interests, and life experiences.',\n",
       "   'index': 0,\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 5, 'completion_tokens': 13, 'total_tokens': 18}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = llm(\"I look forward to\", max_tokens=64, stop=[\"\\n\"], echo=True)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: He opened his eyes and gaspe\n",
      ", time: 18.983027935028076\n",
      "{'id': 'cmpl-4e373640-b1db-4297-bd21-5eb906d14db2', 'object': 'text_completion', 'created': 1697119733, 'model': 'G:\\\\code\\\\pretrain_model_dir\\\\llama-7b\\\\ggml-model-f16.gguf', 'choices': [{'text': 'He opened his eyes and gaspe\\nHe opened his eyes and gasped. His heart was racing fast, he felt as if someone had just hit him over the head with a sledgehammer. The man was shaking from', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 9, 'completion_tokens': 40, 'total_tokens': 49}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: She ran as fast as she coul\n",
      ", time: 12.523684978485107\n",
      "{'id': 'cmpl-7b181436-0d45-46e3-90e0-ad7909077ec5', 'object': 'text_completion', 'created': 1697119752, 'model': 'G:\\\\code\\\\pretrain_model_dir\\\\llama-7b\\\\ggml-model-f16.gguf', 'choices': [{'text': 'She ran as fast as she coul\\nShe ran as fast as she could, but the bicyclist continued to chase her. He caught up with her and threw the chain around her leg. The man then pulled on the chain', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 10, 'completion_tokens': 40, 'total_tokens': 50}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: The phone rang. He ignored i\n",
      ", time: 12.26774549484253\n",
      "{'id': 'cmpl-c545b03d-2dd5-40db-84d9-1496eca0c897', 'object': 'text_completion', 'created': 1697119764, 'model': 'G:\\\\code\\\\pretrain_model_dir\\\\llama-7b\\\\ggml-model-f16.gguf', 'choices': [{'text': 'The phone rang. He ignored i\\nThe phone rang. He ignored it but when the phone rang again, he answered it. It was her mother! She asked him how he felt about marriage and he said that he wanted to marry her', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 9, 'completion_tokens': 40, 'total_tokens': 49}}\n",
      "[18.983027935028076, 12.523684978485107, 12.26774549484253]\n",
      "[40, 40, 40]\n",
      "每秒 token 数: 2.7413246071616566\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "from transformers import LlamaTokenizer, LlamaTokenizerFast\n",
    "\n",
    "model_path = r\"G:\\code\\pretrain_model_dir\\llama-7b-hf\"\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# 记录每次生成的时间和 token 数量\n",
    "time_list = []\n",
    "token_list = []\n",
    "\n",
    "query_list = [\n",
    "    \"I look forward to\",\n",
    "    \"I love beijing , because\",\n",
    "]\n",
    "with open(\"./data/query.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    query_list = f.readlines()[:3]\n",
    "\n",
    "for query in query_list:\n",
    "    start = time.time()\n",
    "    output = llm(query, max_tokens=40, stop=[\"\\n\"], echo=True)\n",
    "    end = time.time()\n",
    "    print(f\"query: {query}, time: {end - start}\")\n",
    "    print(output)\n",
    "\n",
    "    time_list.append(end - start)\n",
    "    inputs = tokenizer(query, return_tensors=\"pt\")\n",
    "    outputs = tokenizer(output[\"choices\"][0][\"text\"], return_tensors=\"pt\")\n",
    "    token_list.append(outputs.input_ids.shape[1] - inputs.input_ids.shape[1])\n",
    "\n",
    "print(time_list)\n",
    "print(token_list)\n",
    "# 计算每秒生成的 token 数量\n",
    "print(\"每秒 token 数:\", sum(token_list) / sum(time_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

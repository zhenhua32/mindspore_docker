{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 需要先转换下模型\n",
    "\n",
    "```bash\n",
    "python .\\convert.py G:\\code\\pretrain_model_dir\\llama-7b\n",
    "\n",
    "$env:CMAKE_ARGS = \"-DLLAMA_CUBLAS=on\"\n",
    "pip install llama-cpp-python --no-cache-dir --verbose\n",
    "```\n",
    "\n",
    "TODO: 感觉好像没跑在 GPU 上, 需要看下文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\tech\\Anaconda3\\envs\\nlp\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Llama in module llama_cpp.llama:\n",
      "\n",
      "class Llama(builtins.object)\n",
      " |  Llama(model_path: str, *, n_gpu_layers: int = 0, main_gpu: int = 0, tensor_split: Union[List[float], NoneType] = None, vocab_only: bool = False, use_mmap: bool = True, use_mlock: bool = False, seed: int = 4294967295, n_ctx: int = 512, n_batch: int = 512, n_threads: Union[int, NoneType] = None, n_threads_batch: Union[int, NoneType] = None, rope_freq_base: float = 0.0, rope_freq_scale: float = 0.0, mul_mat_q: bool = True, f16_kv: bool = True, logits_all: bool = False, embedding: bool = False, last_n_tokens_size: int = 64, lora_base: Union[str, NoneType] = None, lora_scale: float = 1.0, lora_path: Union[str, NoneType] = None, numa: bool = False, chat_format: str = 'llama-2', verbose: bool = True, **kwargs)\n",
      " |  \n",
      " |  High-level Python wrapper for a llama.cpp model.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __call__(self, prompt: str, suffix: Union[str, NoneType] = None, max_tokens: int = 128, temperature: float = 0.8, top_p: float = 0.95, logprobs: Union[int, NoneType] = None, echo: bool = False, stop: Union[str, List[str], NoneType] = [], frequency_penalty: float = 0.0, presence_penalty: float = 0.0, repeat_penalty: float = 1.1, top_k: int = 40, stream: bool = False, tfs_z: float = 1.0, mirostat_mode: int = 0, mirostat_tau: float = 5.0, mirostat_eta: float = 0.1, model: Union[str, NoneType] = None, stopping_criteria: Union[llama_cpp.llama.StoppingCriteriaList, NoneType] = None, logits_processor: Union[llama_cpp.llama.LogitsProcessorList, NoneType] = None, grammar: Union[llama_cpp.llama_grammar.LlamaGrammar, NoneType] = None) -> Union[llama_cpp.llama_types.CreateCompletionResponse, Iterator[llama_cpp.llama_types.CreateCompletionStreamResponse]]\n",
      " |      Generate text from a prompt.\n",
      " |      \n",
      " |      Args:\n",
      " |          prompt: The prompt to generate text from.\n",
      " |          suffix: A suffix to append to the generated text. If None, no suffix is appended.\n",
      " |          max_tokens: The maximum number of tokens to generate. If max_tokens <= 0, the maximum number of tokens to generate is unlimited and depends on n_ctx.\n",
      " |          temperature: The temperature to use for sampling.\n",
      " |          top_p: The top-p value to use for sampling.\n",
      " |          logprobs: The number of logprobs to return. If None, no logprobs are returned.\n",
      " |          echo: Whether to echo the prompt.\n",
      " |          stop: A list of strings to stop generation when encountered.\n",
      " |          repeat_penalty: The penalty to apply to repeated tokens.\n",
      " |          top_k: The top-k value to use for sampling.\n",
      " |          stream: Whether to stream the results.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: If the requested tokens exceed the context window.\n",
      " |          RuntimeError: If the prompt fails to tokenize or the model fails to evaluate the prompt.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Response object containing the generated text.\n",
      " |  \n",
      " |  __del__(self)\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __init__(self, model_path: str, *, n_gpu_layers: int = 0, main_gpu: int = 0, tensor_split: Union[List[float], NoneType] = None, vocab_only: bool = False, use_mmap: bool = True, use_mlock: bool = False, seed: int = 4294967295, n_ctx: int = 512, n_batch: int = 512, n_threads: Union[int, NoneType] = None, n_threads_batch: Union[int, NoneType] = None, rope_freq_base: float = 0.0, rope_freq_scale: float = 0.0, mul_mat_q: bool = True, f16_kv: bool = True, logits_all: bool = False, embedding: bool = False, last_n_tokens_size: int = 64, lora_base: Union[str, NoneType] = None, lora_scale: float = 1.0, lora_path: Union[str, NoneType] = None, numa: bool = False, chat_format: str = 'llama-2', verbose: bool = True, **kwargs)\n",
      " |      Load a llama.cpp model from `model_path`.\n",
      " |      \n",
      " |      Args:\n",
      " |          model_path: Path to the model.\n",
      " |          seed: Random seed. -1 for random.\n",
      " |          n_ctx: Maximum context size.\n",
      " |          n_batch: Maximum number of prompt tokens to batch together when calling llama_eval.\n",
      " |          n_gpu_layers: Number of layers to offload to GPU (-ngl). If -1, all layers are offloaded.\n",
      " |          main_gpu: Main GPU to use.\n",
      " |          tensor_split: Optional list of floats to split the model across multiple GPUs. If None, the model is not split.\n",
      " |          rope_freq_base: Base frequency for rope sampling.\n",
      " |          rope_freq_scale: Scale factor for rope sampling.\n",
      " |          low_vram: Use low VRAM mode.\n",
      " |          mul_mat_q: if true, use experimental mul_mat_q kernels\n",
      " |          f16_kv: Use half-precision for key/value cache.\n",
      " |          logits_all: Return logits for all tokens, not just the last token.\n",
      " |          vocab_only: Only load the vocabulary no weights.\n",
      " |          use_mmap: Use mmap if possible.\n",
      " |          use_mlock: Force the system to keep the model in RAM.\n",
      " |          embedding: Embedding mode only.\n",
      " |          n_threads: Number of threads to use. If None, the number of threads is automatically determined.\n",
      " |          last_n_tokens_size: Maximum number of tokens to keep in the last_n_tokens deque.\n",
      " |          lora_base: Optional path to base model, useful if using a quantized base model and you want to apply LoRA to an f16 model.\n",
      " |          lora_path: Path to a LoRA file to apply to the model.\n",
      " |          numa: Enable NUMA support. (NOTE: The initial value of this parameter is used for the remainder of the program as this value is set in llama_backend_init)\n",
      " |          chat_format: String specifying the chat format to use when calling create_chat_completion.\n",
      " |          verbose: Print verbose output to stderr.\n",
      " |          kwargs: Unused keyword arguments (for additional backwards compatibility).\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: If the model path does not exist.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A Llama instance.\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  create_chat_completion(self, messages: List[llama_cpp.llama_types.ChatCompletionRequestMessage], functions: Union[List[llama_cpp.llama_types.ChatCompletionResponseFunction], NoneType] = None, function_call: Union[str, llama_cpp.llama_types.ChatCompletionFunctionCall, NoneType] = None, temperature: float = 0.2, top_p: float = 0.95, top_k: int = 40, stream: bool = False, stop: Union[str, List[str], NoneType] = [], max_tokens: int = 256, presence_penalty: float = 0.0, frequency_penalty: float = 0.0, repeat_penalty: float = 1.1, tfs_z: float = 1.0, mirostat_mode: int = 0, mirostat_tau: float = 5.0, mirostat_eta: float = 0.1, model: Union[str, NoneType] = None, logits_processor: Union[llama_cpp.llama.LogitsProcessorList, NoneType] = None, grammar: Union[llama_cpp.llama_grammar.LlamaGrammar, NoneType] = None) -> Union[llama_cpp.llama_types.CreateChatCompletionResponse, Iterator[llama_cpp.llama_types.ChatCompletionStreamResponse]]\n",
      " |      Generate a chat completion from a list of messages.\n",
      " |      \n",
      " |      Args:\n",
      " |          messages: A list of messages to generate a response for.\n",
      " |          temperature: The temperature to use for sampling.\n",
      " |          top_p: The top-p value to use for sampling.\n",
      " |          top_k: The top-k value to use for sampling.\n",
      " |          stream: Whether to stream the results.\n",
      " |          stop: A list of strings to stop generation when encountered.\n",
      " |          max_tokens: The maximum number of tokens to generate. If max_tokens <= 0, the maximum number of tokens to generate is unlimited and depends on n_ctx.\n",
      " |          repeat_penalty: The penalty to apply to repeated tokens.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Generated chat completion or a stream of chat completion chunks.\n",
      " |  \n",
      " |  create_completion(self, prompt: str, suffix: Union[str, NoneType] = None, max_tokens: int = 128, temperature: float = 0.8, top_p: float = 0.95, logprobs: Union[int, NoneType] = None, echo: bool = False, stop: Union[str, List[str], NoneType] = [], frequency_penalty: float = 0.0, presence_penalty: float = 0.0, repeat_penalty: float = 1.1, top_k: int = 40, stream: bool = False, tfs_z: float = 1.0, mirostat_mode: int = 0, mirostat_tau: float = 5.0, mirostat_eta: float = 0.1, model: Union[str, NoneType] = None, stopping_criteria: Union[llama_cpp.llama.StoppingCriteriaList, NoneType] = None, logits_processor: Union[llama_cpp.llama.LogitsProcessorList, NoneType] = None, grammar: Union[llama_cpp.llama_grammar.LlamaGrammar, NoneType] = None) -> Union[llama_cpp.llama_types.CreateCompletionResponse, Iterator[llama_cpp.llama_types.CreateCompletionStreamResponse]]\n",
      " |      Generate text from a prompt.\n",
      " |      \n",
      " |      Args:\n",
      " |          prompt: The prompt to generate text from.\n",
      " |          suffix: A suffix to append to the generated text. If None, no suffix is appended.\n",
      " |          max_tokens: The maximum number of tokens to generate. If max_tokens <= 0, the maximum number of tokens to generate is unlimited and depends on n_ctx.\n",
      " |          temperature: The temperature to use for sampling.\n",
      " |          top_p: The top-p value to use for sampling.\n",
      " |          logprobs: The number of logprobs to return. If None, no logprobs are returned.\n",
      " |          echo: Whether to echo the prompt.\n",
      " |          stop: A list of strings to stop generation when encountered.\n",
      " |          repeat_penalty: The penalty to apply to repeated tokens.\n",
      " |          top_k: The top-k value to use for sampling.\n",
      " |          stream: Whether to stream the results.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: If the requested tokens exceed the context window.\n",
      " |          RuntimeError: If the prompt fails to tokenize or the model fails to evaluate the prompt.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Response object containing the generated text.\n",
      " |  \n",
      " |  create_embedding(self, input: Union[str, List[str]], model: Union[str, NoneType] = None) -> llama_cpp.llama_types.CreateEmbeddingResponse\n",
      " |      Embed a string.\n",
      " |      \n",
      " |      Args:\n",
      " |          input: The utf-8 encoded string to embed.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An embedding object.\n",
      " |  \n",
      " |  detokenize(self, tokens: List[int]) -> bytes\n",
      " |      Detokenize a list of tokens.\n",
      " |      \n",
      " |      Args:\n",
      " |          tokens: The list of tokens to detokenize.\n",
      " |      \n",
      " |      Returns:\n",
      " |          The detokenized string.\n",
      " |  \n",
      " |  embed(self, input: str) -> List[float]\n",
      " |      Embed a string.\n",
      " |      \n",
      " |      Args:\n",
      " |          input: The utf-8 encoded string to embed.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A list of embeddings\n",
      " |  \n",
      " |  eval(self, tokens: Sequence[int])\n",
      " |      Evaluate a list of tokens.\n",
      " |      \n",
      " |      Args:\n",
      " |          tokens: The list of tokens to evaluate.\n",
      " |  \n",
      " |  generate(self, tokens: Sequence[int], top_k: int = 40, top_p: float = 0.95, temp: float = 0.8, repeat_penalty: float = 1.1, reset: bool = True, frequency_penalty: float = 0.0, presence_penalty: float = 0.0, tfs_z: float = 1.0, mirostat_mode: int = 0, mirostat_tau: float = 5.0, mirostat_eta: float = 0.1, logits_processor: Union[llama_cpp.llama.LogitsProcessorList, NoneType] = None, stopping_criteria: Union[llama_cpp.llama.StoppingCriteriaList, NoneType] = None, grammar: Union[llama_cpp.llama_grammar.LlamaGrammar, NoneType] = None) -> Generator[int, Union[Sequence[int], NoneType], NoneType]\n",
      " |      Create a generator of tokens from a prompt.\n",
      " |      \n",
      " |      Examples:\n",
      " |          >>> llama = Llama(\"models/ggml-7b.bin\")\n",
      " |          >>> tokens = llama.tokenize(b\"Hello, world!\")\n",
      " |          >>> for token in llama.generate(tokens, top_k=40, top_p=0.95, temp=1.0, repeat_penalty=1.1):\n",
      " |          ...     print(llama.detokenize([token]))\n",
      " |      \n",
      " |      Args:\n",
      " |          tokens: The prompt tokens.\n",
      " |          top_k: The top-k sampling parameter.\n",
      " |          top_p: The top-p sampling parameter.\n",
      " |          temp: The temperature parameter.\n",
      " |          repeat_penalty: The repeat penalty parameter.\n",
      " |          reset: Whether to reset the model state.\n",
      " |      \n",
      " |      Yields:\n",
      " |          The generated tokens.\n",
      " |  \n",
      " |  load_state(self, state: llama_cpp.llama.LlamaState) -> None\n",
      " |  \n",
      " |  n_ctx(self) -> int\n",
      " |      Return the context window size.\n",
      " |  \n",
      " |  n_embd(self) -> int\n",
      " |      Return the embedding size.\n",
      " |  \n",
      " |  n_vocab(self) -> int\n",
      " |      Return the vocabulary size.\n",
      " |  \n",
      " |  reset(self)\n",
      " |      Reset the model state.\n",
      " |  \n",
      " |  sample(self, top_k: int = 40, top_p: float = 0.95, temp: float = 0.8, repeat_penalty: float = 1.1, frequency_penalty: float = 0.0, presence_penalty: float = 0.0, tfs_z: float = 1.0, mirostat_mode: int = 0, mirostat_eta: float = 0.1, mirostat_tau: float = 5.0, penalize_nl: bool = True, logits_processor: Union[llama_cpp.llama.LogitsProcessorList, NoneType] = None, grammar: Union[llama_cpp.llama_grammar.LlamaGrammar, NoneType] = None)\n",
      " |      Sample a token from the model.\n",
      " |      \n",
      " |      Args:\n",
      " |          top_k: The top-k sampling parameter.\n",
      " |          top_p: The top-p sampling parameter.\n",
      " |          temp: The temperature parameter.\n",
      " |          repeat_penalty: The repeat penalty parameter.\n",
      " |      \n",
      " |      Returns:\n",
      " |          The sampled token.\n",
      " |  \n",
      " |  save_state(self) -> llama_cpp.llama.LlamaState\n",
      " |  \n",
      " |  set_cache(self, cache: Union[llama_cpp.llama.BaseLlamaCache, NoneType])\n",
      " |      Set the cache.\n",
      " |      \n",
      " |      Args:\n",
      " |          cache: The cache to set.\n",
      " |  \n",
      " |  token_bos(self) -> int\n",
      " |      Return the beginning-of-sequence token.\n",
      " |  \n",
      " |  token_eos(self) -> int\n",
      " |      Return the end-of-sequence token.\n",
      " |  \n",
      " |  token_nl(self) -> int\n",
      " |      Return the newline token.\n",
      " |  \n",
      " |  tokenize(self, text: bytes, add_bos: bool = True) -> List[int]\n",
      " |      Tokenize a string.\n",
      " |      \n",
      " |      Args:\n",
      " |          text: The utf-8 encoded string to tokenize.\n",
      " |      \n",
      " |      Raises:\n",
      " |          RuntimeError: If the tokenization failed.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A list of tokens.\n",
      " |  \n",
      " |  tokenizer(self) -> 'LlamaTokenizer'\n",
      " |      Return the tokenizer for this model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  logits_to_logprobs(logits: List[float]) -> List[float]\n",
      " |  \n",
      " |  longest_token_prefix(a: Sequence[int], b: Sequence[int])\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  eval_logits\n",
      " |  \n",
      " |  eval_tokens\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Llama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "model_path = r\"G:\\code\\pretrain_model_dir\\llama-7b\\ggml-model-f16.gguf\"\n",
    "llm = Llama(model_path=model_path, n_gpu_layers=-1)\n",
    "# llm = Llama(model_path=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'cmpl-5d383458-75f2-4833-a7e5-89e5430d4bc3',\n",
       " 'object': 'text_completion',\n",
       " 'created': 1697123920,\n",
       " 'model': 'G:\\\\code\\\\pretrain_model_dir\\\\llama-7b\\\\ggml-model-f16.gguf',\n",
       " 'choices': [{'text': \"I look forward to getting back in the studio this month. The holidays are over and it's time to get back into my creative routine. I have been working on a few personal projects that will not\",\n",
       "   'index': 0,\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'length'}],\n",
       " 'usage': {'prompt_tokens': 5, 'completion_tokens': 40, 'total_tokens': 45}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = llm(\"I look forward to\", max_tokens=40, stop=[\"\\n\"], echo=True)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=True`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: He opened his eyes and gaspe\n",
      ", time: 1.07611083984375\n",
      "He opened his eyes and gaspe\n",
      "He opened his eyes and gasped for air, \"It would be good to have a break\". The 21-year old had never played at this level before. and the team will\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: She ran as fast as she coul\n",
      ", time: 0.9678809642791748\n",
      "She ran as fast as she coul\n",
      "She ran as fast as she could until she was out of the woods. She came upon a stream and stopped to catch her breath when she heard a rustle in the trees behind her. “Oh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: The phone rang. He ignored i\n",
      ", time: 0.953073263168335\n",
      "The phone rang. He ignored i\n",
      "The phone rang. He ignored it, and when I asked why he’d been ignoring me for the past few hours (and why I couldn’t talk to him anyway) he said that\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: They met at the airpor\n",
      ", time: 0.7788467407226562\n",
      "They met at the airpor\n",
      "They met at the airport in 2013, She is still wearing it. For all the latest Chandigarh News.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: She loved him. He didn’t kno\n",
      ", time: 0.9673075675964355\n",
      "She loved him. He didn’t kno\n",
      "She loved him. He didn’t know it yet.”I want you to help me get the girls,” Aaron told his friend. “We need a list of people who can go into our\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: He had a secret. A big on\n",
      ", time: 0.9578988552093506\n",
      "He had a secret. A big on\n",
      "He had a secret. A big one. No, it wasn’t his affair with that married woman. Oh no. His wife knew about her and she even encouraged him to cheat on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: She hated her job. But she staye\n",
      ", time: 0.9658300876617432\n",
      "She hated her job. But she staye\n",
      "She hated her job. But she stuck to it because she was convinced that one day she would be recognised and acknowledged for the contribution she made to society, and perhaps even win a Nobel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: The door slammed. He was gon\n",
      ", time: 0.9633538722991943\n",
      "The door slammed. He was gon\n",
      "The door slammed. He was gone. I stood there, my hand to my lips as the tears streamed down my cheeks. My heart sank like a stone into a bottomless pit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: They found the treasure. And the tra\n",
      ", time: 0.8851583003997803\n",
      "They found the treasure. And the tra\n",
      "They found the treasure. And then they got into a fight and fell to their deaths in the ocean. This is what happens to people when they get too much attention.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: He was the last one aliv\n",
      ", time: 0.8598403930664062\n",
      "He was the last one aliv\n",
      "He was the last one alive. A former minister of agriculture and forest,the 1967 World Cup finals in London’s Wembley Stadium.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: She woke up in a strange place\n",
      ", time: 0.9610660076141357\n",
      "She woke up in a strange place\n",
      "She was wearing only her panties, and her hands were tied to the bedposts. It took her a moment before she realized that someone had been there while she slept, and probably\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: He had a plan. A brilliant one\n",
      ", time: 0.9542670249938965\n",
      "He had a plan. A brilliant one\n",
      "When I came to Australia in 1986, my goal was to stay here for about a year or two and then return home - maybe even take up Australian citizenship! The reasons were\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: The letter changed everything\n",
      ", time: 0.45098352432250977\n",
      "The letter changed everything\n",
      "Want to get ahead? Read this letter from the 1800s\n",
      "query: She saw him and smiled\n",
      ", time: 0.1787266731262207\n",
      "She saw him and smiled\n",
      "As her eyes were red\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: He was late. Again\n",
      ", time: 0.9501137733459473\n",
      "He was late. Again\n",
      "I had been waiting for quite a while when I finally decided to go look for him. I could hear his voice all the way from the door of our flat. It was coming from the corrid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: They were trapped. No escape\n",
      ", time: 0.9553947448730469\n",
      "They were trapped. No escape\n",
      "The last of the sun’s rays slid away from the western horizon, bathing the sky in a purple haze as they flew through it, on their way to their rende\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: She couldn’t believe her eyes\n",
      ", time: 0.953341007232666\n",
      "She couldn’t believe her eyes\n",
      "“Oh my God!” she exclaimed. “What a beautiful girl, I thought it was only men who looked like that. She is so cute and beautiful. My goodness…” She continued\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: He heard a scream. He ran\n",
      ", time: 0.9614627361297607\n",
      "He heard a scream. He ran\n",
      "A man walked past the front of the house on Crescent Street. It was late at night, almost dawn. There were no lights in any window but there was one light on in the living\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: They kissed. Fireworks exploded\n",
      ", time: 0.958432674407959\n",
      "They kissed. Fireworks exploded\n",
      "Meghan Markle and Prince Harry kiss on the balcony of Buckingham Palace in London following their wedding Saturday, May 19, 2018. (Jon\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: She had a choice. A hard one\n",
      ", time: 0.9534366130828857\n",
      "She had a choice. A hard one\n",
      "We live in an age of information and technology and yet we see people suffering from diseases and illnesses that could have been avoided if they knew about them ahead of time. As an example,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: He had always wanted to fly\n",
      ", time: 0.9519116878509521\n",
      "He had always wanted to fly\n",
      "He never thought that he could achieve it. But, he had a passion for flying. He wanted to be able to control an aircraft and fly above the ground. He wanted to have complete freedom of\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: She was the best detective in town\n",
      ", time: 0.4541053771972656\n",
      "She was the best detective in town\n",
      "A real-life female Columbo who solved crimes all over the world.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: The war was over. But not for him\n",
      ", time: 0.9566679000854492\n",
      "The war was over. But not for him\n",
      "The Great War had been raging for four years, and the final push had begun on the Somme. The 1st Battalion of the Dorsetshire Regiment was in reserve when it began at\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: She had a gift. A dangerous one\n",
      ", time: 0.9522218704223633\n",
      "She had a gift. A dangerous one\n",
      "Mary Magdalene was born around 10 BC, at the same time as Julius Caesar and Herod the Great were running roughshod over the Middle East. Her birthplace was\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: He didn’t expect to find love\n",
      ", time: 0.9064345359802246\n",
      "He didn’t expect to find love\n",
      "After his wife died, he turned himself into a recluse. But when the most beautiful woman in town asked him out on a date, he couldn't deny her.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: She was lost in the woods\n",
      ", time: 0.6361308097839355\n",
      "She was lost in the woods\n",
      "Sep. 16, 2014 | 7:05 am | Author : Jessica May\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: The virus was spreading. Fast\n",
      ", time: 0.9661595821380615\n",
      "The virus was spreading. Fast\n",
      "In the past two years, 500 people have died from Lassa fever in Nigeria, and the virus has now been identified outside the country for the first time. A man who\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: He had a mission. A secret one\n",
      ", time: 0.5708303451538086\n",
      "He had a mission. A secret one\n",
      "He was only supposed to be there for an hour or two, but in the end he stayed all day.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: She was a princess. But not by choice\n",
      ", time: 0.47647523880004883\n",
      "She was a princess. But not by choice\n",
      "Maria Teresa Carafa-Spinelli died Monday at age 79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: He was a thief. A master one\n",
      ", time: 0.9555888175964355\n",
      "He was a thief. A master one\n",
      "A master of the art, and a real professional. There are thieves who steal from their neighbors, and there are professionals who steal for the sake of it. The latter is\n",
      "[1.07611083984375, 0.9678809642791748, 0.953073263168335, 0.7788467407226562, 0.9673075675964355, 0.9578988552093506, 0.9658300876617432, 0.9633538722991943, 0.8851583003997803, 0.8598403930664062, 0.9610660076141357, 0.9542670249938965, 0.45098352432250977, 0.1787266731262207, 0.9501137733459473, 0.9553947448730469, 0.953341007232666, 0.9614627361297607, 0.958432674407959, 0.9534366130828857, 0.9519116878509521, 0.4541053771972656, 0.9566679000854492, 0.9522218704223633, 0.9064345359802246, 0.6361308097839355, 0.9661595821380615, 0.5708303451538086, 0.47647523880004883, 0.9555888175964355]\n",
      "[40, 40, 40, 31, 40, 40, 40, 40, 36, 35, 40, 40, 17, 5, 40, 40, 40, 40, 40, 40, 40, 17, 40, 40, 37, 25, 40, 22, 18, 40]\n",
      "每秒 token 数: 40.93558924504671\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "from transformers import LlamaTokenizer, LlamaTokenizerFast\n",
    "\n",
    "model_path = r\"G:\\code\\pretrain_model_dir\\llama-7b-hf\"\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# 记录每次生成的时间和 token 数量\n",
    "time_list = []\n",
    "token_list = []\n",
    "\n",
    "query_list = [\n",
    "    \"I look forward to\",\n",
    "    \"I love beijing , because\",\n",
    "]\n",
    "with open(\"./data/query.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    query_list = f.readlines()\n",
    "\n",
    "for query in query_list:\n",
    "    start = time.time()\n",
    "    output = llm(query, max_tokens=40, stop=[\"\\n\"], echo=True)\n",
    "    end = time.time()\n",
    "    print(f\"query: {query}, time: {end - start}\")\n",
    "    print(output[\"choices\"][0][\"text\"])\n",
    "\n",
    "    time_list.append(end - start)\n",
    "    inputs = tokenizer(query, return_tensors=\"pt\")\n",
    "    outputs = tokenizer(output[\"choices\"][0][\"text\"], return_tensors=\"pt\")\n",
    "    token_list.append(outputs.input_ids.shape[1] - inputs.input_ids.shape[1])\n",
    "\n",
    "print(time_list)\n",
    "print(token_list)\n",
    "# 计算每秒生成的 token 数量\n",
    "print(\"每秒 token 数:\", sum(token_list) / sum(time_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

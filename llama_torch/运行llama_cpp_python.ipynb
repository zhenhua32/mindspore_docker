{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 需要先转换下模型\n",
    "\n",
    "```bash\n",
    "python .\\convert.py G:\\code\\pretrain_model_dir\\llama-7b\n",
    "\n",
    "$env:CMAKE_ARGS = \"-DLLAMA_CUBLAS=on\"\n",
    "pip install llama-cpp-python --no-cache-dir --verbose\n",
    "```\n",
    "\n",
    "TODO: 感觉好像没跑在 GPU 上, 需要看下文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Llama in module llama_cpp.llama:\n",
      "\n",
      "class Llama(builtins.object)\n",
      " |  Llama(model_path: str, *, n_gpu_layers: int = 0, main_gpu: int = 0, tensor_split: Union[List[float], NoneType] = None, vocab_only: bool = False, use_mmap: bool = True, use_mlock: bool = False, seed: int = 4294967295, n_ctx: int = 512, n_batch: int = 512, n_threads: Union[int, NoneType] = None, n_threads_batch: Union[int, NoneType] = None, rope_freq_base: float = 0.0, rope_freq_scale: float = 0.0, mul_mat_q: bool = True, f16_kv: bool = True, logits_all: bool = False, embedding: bool = False, last_n_tokens_size: int = 64, lora_base: Union[str, NoneType] = None, lora_scale: float = 1.0, lora_path: Union[str, NoneType] = None, numa: bool = False, chat_format: str = 'llama-2', verbose: bool = True, **kwargs)\n",
      " |  \n",
      " |  High-level Python wrapper for a llama.cpp model.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __call__(self, prompt: str, suffix: Union[str, NoneType] = None, max_tokens: int = 128, temperature: float = 0.8, top_p: float = 0.95, logprobs: Union[int, NoneType] = None, echo: bool = False, stop: Union[str, List[str], NoneType] = [], frequency_penalty: float = 0.0, presence_penalty: float = 0.0, repeat_penalty: float = 1.1, top_k: int = 40, stream: bool = False, tfs_z: float = 1.0, mirostat_mode: int = 0, mirostat_tau: float = 5.0, mirostat_eta: float = 0.1, model: Union[str, NoneType] = None, stopping_criteria: Union[llama_cpp.llama.StoppingCriteriaList, NoneType] = None, logits_processor: Union[llama_cpp.llama.LogitsProcessorList, NoneType] = None, grammar: Union[llama_cpp.llama_grammar.LlamaGrammar, NoneType] = None) -> Union[llama_cpp.llama_types.CreateCompletionResponse, Iterator[llama_cpp.llama_types.CreateCompletionStreamResponse]]\n",
      " |      Generate text from a prompt.\n",
      " |      \n",
      " |      Args:\n",
      " |          prompt: The prompt to generate text from.\n",
      " |          suffix: A suffix to append to the generated text. If None, no suffix is appended.\n",
      " |          max_tokens: The maximum number of tokens to generate. If max_tokens <= 0, the maximum number of tokens to generate is unlimited and depends on n_ctx.\n",
      " |          temperature: The temperature to use for sampling.\n",
      " |          top_p: The top-p value to use for sampling.\n",
      " |          logprobs: The number of logprobs to return. If None, no logprobs are returned.\n",
      " |          echo: Whether to echo the prompt.\n",
      " |          stop: A list of strings to stop generation when encountered.\n",
      " |          repeat_penalty: The penalty to apply to repeated tokens.\n",
      " |          top_k: The top-k value to use for sampling.\n",
      " |          stream: Whether to stream the results.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: If the requested tokens exceed the context window.\n",
      " |          RuntimeError: If the prompt fails to tokenize or the model fails to evaluate the prompt.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Response object containing the generated text.\n",
      " |  \n",
      " |  __del__(self)\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __init__(self, model_path: str, *, n_gpu_layers: int = 0, main_gpu: int = 0, tensor_split: Union[List[float], NoneType] = None, vocab_only: bool = False, use_mmap: bool = True, use_mlock: bool = False, seed: int = 4294967295, n_ctx: int = 512, n_batch: int = 512, n_threads: Union[int, NoneType] = None, n_threads_batch: Union[int, NoneType] = None, rope_freq_base: float = 0.0, rope_freq_scale: float = 0.0, mul_mat_q: bool = True, f16_kv: bool = True, logits_all: bool = False, embedding: bool = False, last_n_tokens_size: int = 64, lora_base: Union[str, NoneType] = None, lora_scale: float = 1.0, lora_path: Union[str, NoneType] = None, numa: bool = False, chat_format: str = 'llama-2', verbose: bool = True, **kwargs)\n",
      " |      Load a llama.cpp model from `model_path`.\n",
      " |      \n",
      " |      Args:\n",
      " |          model_path: Path to the model.\n",
      " |          seed: Random seed. -1 for random.\n",
      " |          n_ctx: Maximum context size.\n",
      " |          n_batch: Maximum number of prompt tokens to batch together when calling llama_eval.\n",
      " |          n_gpu_layers: Number of layers to offload to GPU (-ngl). If -1, all layers are offloaded.\n",
      " |          main_gpu: Main GPU to use.\n",
      " |          tensor_split: Optional list of floats to split the model across multiple GPUs. If None, the model is not split.\n",
      " |          rope_freq_base: Base frequency for rope sampling.\n",
      " |          rope_freq_scale: Scale factor for rope sampling.\n",
      " |          low_vram: Use low VRAM mode.\n",
      " |          mul_mat_q: if true, use experimental mul_mat_q kernels\n",
      " |          f16_kv: Use half-precision for key/value cache.\n",
      " |          logits_all: Return logits for all tokens, not just the last token.\n",
      " |          vocab_only: Only load the vocabulary no weights.\n",
      " |          use_mmap: Use mmap if possible.\n",
      " |          use_mlock: Force the system to keep the model in RAM.\n",
      " |          embedding: Embedding mode only.\n",
      " |          n_threads: Number of threads to use. If None, the number of threads is automatically determined.\n",
      " |          last_n_tokens_size: Maximum number of tokens to keep in the last_n_tokens deque.\n",
      " |          lora_base: Optional path to base model, useful if using a quantized base model and you want to apply LoRA to an f16 model.\n",
      " |          lora_path: Path to a LoRA file to apply to the model.\n",
      " |          numa: Enable NUMA support. (NOTE: The initial value of this parameter is used for the remainder of the program as this value is set in llama_backend_init)\n",
      " |          chat_format: String specifying the chat format to use when calling create_chat_completion.\n",
      " |          verbose: Print verbose output to stderr.\n",
      " |          kwargs: Unused keyword arguments (for additional backwards compatibility).\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: If the model path does not exist.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A Llama instance.\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  create_chat_completion(self, messages: List[llama_cpp.llama_types.ChatCompletionRequestMessage], functions: Union[List[llama_cpp.llama_types.ChatCompletionResponseFunction], NoneType] = None, function_call: Union[str, llama_cpp.llama_types.ChatCompletionFunctionCall, NoneType] = None, temperature: float = 0.2, top_p: float = 0.95, top_k: int = 40, stream: bool = False, stop: Union[str, List[str], NoneType] = [], max_tokens: int = 256, presence_penalty: float = 0.0, frequency_penalty: float = 0.0, repeat_penalty: float = 1.1, tfs_z: float = 1.0, mirostat_mode: int = 0, mirostat_tau: float = 5.0, mirostat_eta: float = 0.1, model: Union[str, NoneType] = None, logits_processor: Union[llama_cpp.llama.LogitsProcessorList, NoneType] = None, grammar: Union[llama_cpp.llama_grammar.LlamaGrammar, NoneType] = None) -> Union[llama_cpp.llama_types.CreateChatCompletionResponse, Iterator[llama_cpp.llama_types.ChatCompletionStreamResponse]]\n",
      " |      Generate a chat completion from a list of messages.\n",
      " |      \n",
      " |      Args:\n",
      " |          messages: A list of messages to generate a response for.\n",
      " |          temperature: The temperature to use for sampling.\n",
      " |          top_p: The top-p value to use for sampling.\n",
      " |          top_k: The top-k value to use for sampling.\n",
      " |          stream: Whether to stream the results.\n",
      " |          stop: A list of strings to stop generation when encountered.\n",
      " |          max_tokens: The maximum number of tokens to generate. If max_tokens <= 0, the maximum number of tokens to generate is unlimited and depends on n_ctx.\n",
      " |          repeat_penalty: The penalty to apply to repeated tokens.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Generated chat completion or a stream of chat completion chunks.\n",
      " |  \n",
      " |  create_completion(self, prompt: str, suffix: Union[str, NoneType] = None, max_tokens: int = 128, temperature: float = 0.8, top_p: float = 0.95, logprobs: Union[int, NoneType] = None, echo: bool = False, stop: Union[str, List[str], NoneType] = [], frequency_penalty: float = 0.0, presence_penalty: float = 0.0, repeat_penalty: float = 1.1, top_k: int = 40, stream: bool = False, tfs_z: float = 1.0, mirostat_mode: int = 0, mirostat_tau: float = 5.0, mirostat_eta: float = 0.1, model: Union[str, NoneType] = None, stopping_criteria: Union[llama_cpp.llama.StoppingCriteriaList, NoneType] = None, logits_processor: Union[llama_cpp.llama.LogitsProcessorList, NoneType] = None, grammar: Union[llama_cpp.llama_grammar.LlamaGrammar, NoneType] = None) -> Union[llama_cpp.llama_types.CreateCompletionResponse, Iterator[llama_cpp.llama_types.CreateCompletionStreamResponse]]\n",
      " |      Generate text from a prompt.\n",
      " |      \n",
      " |      Args:\n",
      " |          prompt: The prompt to generate text from.\n",
      " |          suffix: A suffix to append to the generated text. If None, no suffix is appended.\n",
      " |          max_tokens: The maximum number of tokens to generate. If max_tokens <= 0, the maximum number of tokens to generate is unlimited and depends on n_ctx.\n",
      " |          temperature: The temperature to use for sampling.\n",
      " |          top_p: The top-p value to use for sampling.\n",
      " |          logprobs: The number of logprobs to return. If None, no logprobs are returned.\n",
      " |          echo: Whether to echo the prompt.\n",
      " |          stop: A list of strings to stop generation when encountered.\n",
      " |          repeat_penalty: The penalty to apply to repeated tokens.\n",
      " |          top_k: The top-k value to use for sampling.\n",
      " |          stream: Whether to stream the results.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: If the requested tokens exceed the context window.\n",
      " |          RuntimeError: If the prompt fails to tokenize or the model fails to evaluate the prompt.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Response object containing the generated text.\n",
      " |  \n",
      " |  create_embedding(self, input: Union[str, List[str]], model: Union[str, NoneType] = None) -> llama_cpp.llama_types.CreateEmbeddingResponse\n",
      " |      Embed a string.\n",
      " |      \n",
      " |      Args:\n",
      " |          input: The utf-8 encoded string to embed.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An embedding object.\n",
      " |  \n",
      " |  detokenize(self, tokens: List[int]) -> bytes\n",
      " |      Detokenize a list of tokens.\n",
      " |      \n",
      " |      Args:\n",
      " |          tokens: The list of tokens to detokenize.\n",
      " |      \n",
      " |      Returns:\n",
      " |          The detokenized string.\n",
      " |  \n",
      " |  embed(self, input: str) -> List[float]\n",
      " |      Embed a string.\n",
      " |      \n",
      " |      Args:\n",
      " |          input: The utf-8 encoded string to embed.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A list of embeddings\n",
      " |  \n",
      " |  eval(self, tokens: Sequence[int])\n",
      " |      Evaluate a list of tokens.\n",
      " |      \n",
      " |      Args:\n",
      " |          tokens: The list of tokens to evaluate.\n",
      " |  \n",
      " |  generate(self, tokens: Sequence[int], top_k: int = 40, top_p: float = 0.95, temp: float = 0.8, repeat_penalty: float = 1.1, reset: bool = True, frequency_penalty: float = 0.0, presence_penalty: float = 0.0, tfs_z: float = 1.0, mirostat_mode: int = 0, mirostat_tau: float = 5.0, mirostat_eta: float = 0.1, logits_processor: Union[llama_cpp.llama.LogitsProcessorList, NoneType] = None, stopping_criteria: Union[llama_cpp.llama.StoppingCriteriaList, NoneType] = None, grammar: Union[llama_cpp.llama_grammar.LlamaGrammar, NoneType] = None) -> Generator[int, Union[Sequence[int], NoneType], NoneType]\n",
      " |      Create a generator of tokens from a prompt.\n",
      " |      \n",
      " |      Examples:\n",
      " |          >>> llama = Llama(\"models/ggml-7b.bin\")\n",
      " |          >>> tokens = llama.tokenize(b\"Hello, world!\")\n",
      " |          >>> for token in llama.generate(tokens, top_k=40, top_p=0.95, temp=1.0, repeat_penalty=1.1):\n",
      " |          ...     print(llama.detokenize([token]))\n",
      " |      \n",
      " |      Args:\n",
      " |          tokens: The prompt tokens.\n",
      " |          top_k: The top-k sampling parameter.\n",
      " |          top_p: The top-p sampling parameter.\n",
      " |          temp: The temperature parameter.\n",
      " |          repeat_penalty: The repeat penalty parameter.\n",
      " |          reset: Whether to reset the model state.\n",
      " |      \n",
      " |      Yields:\n",
      " |          The generated tokens.\n",
      " |  \n",
      " |  load_state(self, state: llama_cpp.llama.LlamaState) -> None\n",
      " |  \n",
      " |  n_ctx(self) -> int\n",
      " |      Return the context window size.\n",
      " |  \n",
      " |  n_embd(self) -> int\n",
      " |      Return the embedding size.\n",
      " |  \n",
      " |  n_vocab(self) -> int\n",
      " |      Return the vocabulary size.\n",
      " |  \n",
      " |  reset(self)\n",
      " |      Reset the model state.\n",
      " |  \n",
      " |  sample(self, top_k: int = 40, top_p: float = 0.95, temp: float = 0.8, repeat_penalty: float = 1.1, frequency_penalty: float = 0.0, presence_penalty: float = 0.0, tfs_z: float = 1.0, mirostat_mode: int = 0, mirostat_eta: float = 0.1, mirostat_tau: float = 5.0, penalize_nl: bool = True, logits_processor: Union[llama_cpp.llama.LogitsProcessorList, NoneType] = None, grammar: Union[llama_cpp.llama_grammar.LlamaGrammar, NoneType] = None)\n",
      " |      Sample a token from the model.\n",
      " |      \n",
      " |      Args:\n",
      " |          top_k: The top-k sampling parameter.\n",
      " |          top_p: The top-p sampling parameter.\n",
      " |          temp: The temperature parameter.\n",
      " |          repeat_penalty: The repeat penalty parameter.\n",
      " |      \n",
      " |      Returns:\n",
      " |          The sampled token.\n",
      " |  \n",
      " |  save_state(self) -> llama_cpp.llama.LlamaState\n",
      " |  \n",
      " |  set_cache(self, cache: Union[llama_cpp.llama.BaseLlamaCache, NoneType])\n",
      " |      Set the cache.\n",
      " |      \n",
      " |      Args:\n",
      " |          cache: The cache to set.\n",
      " |  \n",
      " |  token_bos(self) -> int\n",
      " |      Return the beginning-of-sequence token.\n",
      " |  \n",
      " |  token_eos(self) -> int\n",
      " |      Return the end-of-sequence token.\n",
      " |  \n",
      " |  token_nl(self) -> int\n",
      " |      Return the newline token.\n",
      " |  \n",
      " |  tokenize(self, text: bytes, add_bos: bool = True) -> List[int]\n",
      " |      Tokenize a string.\n",
      " |      \n",
      " |      Args:\n",
      " |          text: The utf-8 encoded string to tokenize.\n",
      " |      \n",
      " |      Raises:\n",
      " |          RuntimeError: If the tokenization failed.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A list of tokens.\n",
      " |  \n",
      " |  tokenizer(self) -> 'LlamaTokenizer'\n",
      " |      Return the tokenizer for this model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  logits_to_logprobs(logits: List[float]) -> List[float]\n",
      " |  \n",
      " |  longest_token_prefix(a: Sequence[int], b: Sequence[int])\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  eval_logits\n",
      " |  \n",
      " |  eval_tokens\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Llama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "model_path = r\"G:\\code\\pretrain_model_dir\\llama-7b\\ggml-model-f16.gguf\"\n",
    "llm = Llama(model_path=model_path, n_gpu_layers=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'cmpl-6e3205c9-9140-4671-b5a5-c62ce0877328',\n",
       " 'object': 'text_completion',\n",
       " 'created': 1697122412,\n",
       " 'model': 'G:\\\\code\\\\pretrain_model_dir\\\\llama-7b\\\\ggml-model-f16.gguf',\n",
       " 'choices': [{'text': 'I look forward to meeting you at our wedding!',\n",
       "   'index': 0,\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 5, 'completion_tokens': 8, 'total_tokens': 13}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = llm(\"I look forward to\", max_tokens=40, stop=[\"\\n\"], echo=True)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: He opened his eyes and gaspe\n",
      ", time: 1.1316921710968018\n",
      "He opened his eyes and gaspe\n",
      "He opened his eyes and gasped. What the hell was that? He saw a white light flashing in front of him, he looked up and saw some kind of flying object. He heard\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: She ran as fast as she coul\n",
      ", time: 0.9549820423126221\n",
      "She ran as fast as she coul\n",
      "She ran as fast as she could, and when she got to the end of the street she stopped. She looked around for a little while but didn’t see anyone. In fact, no one\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: The phone rang. He ignored i\n",
      ", time: 0.7493360042572021\n",
      "The phone rang. He ignored i\n",
      "The phone rang. He ignored it and continued to watch the game. It rang again, and this time he picked up the phone. “Yes?”\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: They met at the airpor\n",
      ", time: 0.9612278938293457\n",
      "They met at the airpor\n",
      "They met at the airport, which would make it much easier for them to maintain their relationship. 1834: The first railway engine, “The Rocket”, was invented by\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: She loved him. He didn’t kno\n",
      ", time: 0.9492988586425781\n",
      "She loved him. He didn’t kno\n",
      "She loved him. He didn’t know what else to do, but turn and face her, burying his nose in the scent of her hair. “I can make it up to\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: He had a secret. A big on\n",
      ", time: 0.9490706920623779\n",
      "He had a secret. A big on\n",
      "He had a secret. A big one, but not the kind that he was going to share with anyone. Not tonight. As his body lay motionless in bed, his heart pumped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: She hated her job. But she staye\n",
      ", time: 0.9651272296905518\n",
      "She hated her job. But she staye\n",
      "She hated her job. But she stayed because it was the only one available for a single mom who needed to provide for herself and her four children, ages five through 13 years old.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: The door slammed. He was gon\n",
      ", time: 0.9527075290679932\n",
      "The door slammed. He was gon\n",
      "The door slammed. He was gone, and I didn’t know where the hell he had run off to. I didn’t even think about looking for him—I just knew he would\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: They found the treasure. And the tra\n",
      ", time: 0.885735034942627\n",
      "They found the treasure. And the tra\n",
      "When they awoke, the sun had just disappeared into the horizon and, without warning, he was alone in his bedroom with no one else to be seen or heard.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: He was the last one aliv\n",
      ", time: 0.9589717388153076\n",
      "He was the last one aliv\n",
      "He was the last one alive in his family, 2017 1:34 am In a letter to CEC, The petitioners alleged that on October 9.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: She woke up in a strange place\n",
      ", time: 0.9472537040710449\n",
      "She woke up in a strange place\n",
      "It’s hard to imagine that someone can be so cruel. This is what happened when one young lady got drunk and passed out on the floor at a party she attended with her friends. To\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: He had a plan. A brilliant one\n",
      ", time: 0.954035758972168\n",
      "He had a plan. A brilliant one\n",
      "NASA Administrator James Webb has an idea for solving the shortfall of money that will be needed to maintain America's spaceflight capabilities in the year 2008.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: The letter changed everything\n",
      ", time: 0.9653947353363037\n",
      "The letter changed everything\n",
      "“The thing I learned from this was that I could help people,” said John Sharpe, ’64. “There is a real need to teach, and the more you teach, the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: She saw him and smiled\n",
      ", time: 0.7694878578186035\n",
      "She saw him and smiled\n",
      "July 1, 2014 July 1, 2014 / Katie Sluiter\t/ Leave a comment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: He was late. Again\n",
      ", time: 0.9547691345214844\n",
      "He was late. Again\n",
      "She was waiting, but she still had time to sit at the table and look through a couple of papers. The phone rang; it was him. She tried to appear excited about his call. ‘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: They were trapped. No escape\n",
      ", time: 0.5894136428833008\n",
      "They were trapped. No escape\n",
      "Neglected for years, the old house had become an unloved place of sadness and neglect.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: She couldn’t believe her eyes\n",
      ", time: 0.9366207122802734\n",
      "She couldn’t believe her eyes\n",
      "“Mother of God!” she exclaimed. “It was a dead man! And what was more, he had been buried in a shroud and his grave had not yet been opened.”\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: He heard a scream. He ran\n",
      ", time: 0.9536046981811523\n",
      "He heard a scream. He ran\n",
      "He heard a scream. He ran outside his home at 3:10AM and saw a woman being raped in front of his house. He fought off the rapist, got a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: They kissed. Fireworks exploded\n",
      ", time: 0.9619050025939941\n",
      "They kissed. Fireworks exploded\n",
      "She was an old biddy living next door. The widow of a wealthy, successful man who had died in the crash of 1929. He left her with more money\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: She had a choice. A hard one\n",
      ", time: 0.9590346813201904\n",
      "She had a choice. A hard one\n",
      "The woman in the picture is holding up a piece of paper with a sign saying “My name is Martha and I want abortion.” Her expression is intense, determined but vulnerable.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: He had always wanted to fly\n",
      ", time: 0.9631884098052979\n",
      "He had always wanted to fly\n",
      "He had always wanted to fly. Now he was on his way. The last weekend, he’d gotten the chance to drive a biplane over to the airfield and give it a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: She was the best detective in town\n",
      ", time: 0.3790779113769531\n",
      "She was the best detective in town\n",
      "So they said, and that’s why she got the job.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: The war was over. But not for him\n",
      ", time: 0.9321498870849609\n",
      "The war was over. But not for him\n",
      "By John Huggan 18 Jul 2014, 1:36 pm Updated: 5 Jun 2016, 7:09 am\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: She had a gift. A dangerous one\n",
      ", time: 0.9551591873168945\n",
      "She had a gift. A dangerous one\n",
      "As a young girl, the future Maeve Brennan was sent by her father from Ireland to live with her family in New York and it is here she will spend most of her life.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: He didn’t expect to find love\n",
      ", time: 0.9551372528076172\n",
      "He didn’t expect to find love\n",
      "When he was 60 years old, my father (87) met a woman who was going through the same problems that he had. They married and were together for five years until she died\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: She was lost in the woods\n",
      ", time: 0.959038257598877\n",
      "She was lost in the woods\n",
      "A long time ago there lived a woman who had two daughters. They were so poor that they could hardly get by and often went without food. Then one day one of them got sick and nothing helped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: The virus was spreading. Fast\n",
      ", time: 0.9589338302612305\n",
      "The virus was spreading. Fast\n",
      "The new coronavirus is a mutant, one that has leapt from animal to human and now seems to be able to jump from person-to-person as well. It kills up\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: He had a mission. A secret one\n",
      ", time: 0.4280893802642822\n",
      "He had a mission. A secret one\n",
      "Grabbing the arm of his friend, he shook him awake.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: She was a princess. But not by choice\n",
      ", time: 0.962332010269165\n",
      "She was a princess. But not by choice\n",
      "\"I'll tell you what happens to the daughter of Queen Victoria,\" says Diana Mishkova, \"She becomes a prisoner in her own home.\" For Diana and her sister Maria\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: He was a thief. A master one\n",
      ", time: 0.954064130783081\n",
      "He was a thief. A master one\n",
      "The man who was in the lead had his back turned to him but he didn't feel he should risk waiting any longer for things to get worse - and so he leaped forward at the man\n",
      "[1.1316921710968018, 0.9549820423126221, 0.7493360042572021, 0.9612278938293457, 0.9492988586425781, 0.9490706920623779, 0.9651272296905518, 0.9527075290679932, 0.885735034942627, 0.9589717388153076, 0.9472537040710449, 0.954035758972168, 0.9653947353363037, 0.7694878578186035, 0.9547691345214844, 0.5894136428833008, 0.9366207122802734, 0.9536046981811523, 0.9619050025939941, 0.9590346813201904, 0.9631884098052979, 0.3790779113769531, 0.9321498870849609, 0.9551591873168945, 0.9551372528076172, 0.959038257598877, 0.9589338302612305, 0.4280893802642822, 0.962332010269165, 0.954064130783081]\n",
      "[40, 40, 30, 40, 40, 40, 40, 40, 36, 40, 40, 40, 40, 31, 40, 23, 38, 40, 40, 39, 40, 14, 38, 40, 40, 40, 40, 16, 40, 40]\n",
      "每秒 token 数: 41.08289395559243\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "from transformers import LlamaTokenizer, LlamaTokenizerFast\n",
    "\n",
    "model_path = r\"G:\\code\\pretrain_model_dir\\llama-7b-hf\"\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# 记录每次生成的时间和 token 数量\n",
    "time_list = []\n",
    "token_list = []\n",
    "\n",
    "query_list = [\n",
    "    \"I look forward to\",\n",
    "    \"I love beijing , because\",\n",
    "]\n",
    "with open(\"./data/query.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    query_list = f.readlines()\n",
    "\n",
    "for query in query_list:\n",
    "    start = time.time()\n",
    "    output = llm(query, max_tokens=40, stop=[\"\\n\"], echo=True)\n",
    "    end = time.time()\n",
    "    print(f\"query: {query}, time: {end - start}\")\n",
    "    print(output[\"choices\"][0][\"text\"])\n",
    "\n",
    "    time_list.append(end - start)\n",
    "    inputs = tokenizer(query, return_tensors=\"pt\")\n",
    "    outputs = tokenizer(output[\"choices\"][0][\"text\"], return_tensors=\"pt\")\n",
    "    token_list.append(outputs.input_ids.shape[1] - inputs.input_ids.shape[1])\n",
    "\n",
    "print(time_list)\n",
    "print(token_list)\n",
    "# 计算每秒生成的 token 数量\n",
    "print(\"每秒 token 数:\", sum(token_list) / sum(time_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

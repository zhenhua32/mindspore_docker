{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\tech\\Anaconda3\\envs\\nlp\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM, LlamaTokenizerFast\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=True`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>\n",
      "LlamaTokenizer(name_or_path='G:\\code\\pretrain_model_dir\\llama-7b', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True)}, clean_up_tokenization_spaces=False)\n"
     ]
    }
   ],
   "source": [
    "model_path = r\"G:\\code\\pretrain_model_dir\\llama-7b\"\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_path)\n",
    "print(type(tokenizer))\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='G:\\code\\pretrain_model_dir\\llama-7b', vocab_size=32000, model_max_length=2048, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True)}, clean_up_tokenization_spaces=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.bos_token_id)\n",
    "print(tokenizer.eos_token_id)\n",
    "print(tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:18<00:00,  9.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float16 cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 加载模型\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    model_path, torch_dtype=torch.float16, device_map='auto',\n",
    ")\n",
    "print(model.dtype, model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I look forward to the day when I can say that I have a full-time job. I’m not there yet, but I’m getting closer.\\nI’m a freelance writer and editor']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"I look forward to\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=40)\n",
    "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I love beijing , because it is a city with a lot of history and culture.\\nI love beijing because it is a city with a lot of history and culture.\\nI love beijing because it is']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"I love beijing , because\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=40)\n",
    "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试推理速度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: He opened his eyes and gaspe\n",
      ", time: 1.241760015487671\n",
      "['He opened his eyes and gaspe\\nHe opened his eyes and gasped. \"I\\'m sorry, I\\'m sorry, I\\'m sorry,\" he said. \"I\\'m sorry, I\\'m sorry, I']\n",
      "query: She ran as fast as she coul\n",
      ", time: 1.1983413696289062\n",
      "['She ran as fast as she coul\\nShe ran as fast as she could, but she couldn’t catch up with the train.\\nShe was so sad that she cried.\\nShe cried so hard that she couldn’t see.']\n",
      "query: The phone rang. He ignored i\n",
      ", time: 1.1540663242340088\n",
      "['The phone rang. He ignored i\\nThe phone rang. He ignored it. He was in the middle of a game of chess with his son.\\n\"Dad, it\\'s for you.\"\\n\"I\\'m busy']\n",
      "query: They met at the airpor\n",
      ", time: 1.1993374824523926\n",
      "['They met at the airpor\\nThey met at the airport and were married in a civil ceremony in 2011.\\nThe couple have a daughter, 10-month-old North West.\\nKim']\n",
      "query: She loved him. He didn’t kno\n",
      ", time: 1.246180534362793\n",
      "['She loved him. He didn’t kno\\nShe loved him. He didn’t know it, but he was the first man she’d ever loved. She’d never told him. She’d never told anyone. She’d']\n",
      "query: He had a secret. A big on\n",
      ", time: 1.1699597835540771\n",
      "['He had a secret. A big on\\nHe had a secret. A big one. He was a spy. He was a spy for the United States. He was a spy for the United States. He was a spy']\n",
      "query: She hated her job. But she staye\n",
      ", time: 1.1699461936950684\n",
      "['She hated her job. But she staye\\nShe hated her job. But she stayed because she had no choice. She was a single mother with two children. She was a single mother with two children. She was a single mother with two']\n",
      "query: The door slammed. He was gon\n",
      ", time: 1.186004400253296\n",
      "['The door slammed. He was gon\\nThe door slammed. He was gone.\\nI was alone in the house.\\nI was alone in the house. I was alone in the house. I was alone in the house. I']\n",
      "query: They found the treasure. And the tra\n",
      ", time: 1.1689057350158691\n",
      "['They found the treasure. And the tra\\nThey found the treasure. And the treasure was a map.\\nThe map was a map of the world.\\nThe map was a map of the world. The map was a map of']\n",
      "query: He was the last one aliv\n",
      ", time: 2.9811220169067383\n",
      "['He was the last one aliv\\nHe was the last one alive.\\nThe 2016-17 season was a disaster for the club. The team finished 10th in the league and was knocked']\n",
      "query: She woke up in a strange place\n",
      ", time: 3.141934394836426\n",
      "[\"She woke up in a strange place\\nShe woke up in a strange place,\\nShe didn't know where she was,\\nShe didn't know what she was doing there,\\nShe didn't know what she was\"]\n",
      "query: He had a plan. A brilliant one\n",
      ", time: 3.103708505630493\n",
      "['He had a plan. A brilliant one\\nHe had a plan. A brilliant one. He would take the money he had saved up and buy a house. He would buy a house in a nice neighborhood, with a big yard and a pool']\n",
      "query: The letter changed everything\n",
      ", time: 3.046123504638672\n",
      "['The letter changed everything\\nI was a 17-year-old high school senior when I received a letter from the University of Michigan. It was a rejection letter.\\nI was a 17-year']\n",
      "query: She saw him and smiled\n",
      ", time: 3.026996612548828\n",
      "['She saw him and smiled\\nShe saw him and smiled.\\nHe was a man of few words,\\nBut she knew he was a man of few words.\\nShe knew he was a man of few words.\\nShe']\n",
      "query: He was late. Again\n",
      ", time: 3.011507749557495\n",
      "['He was late. Again\\nI was late. Again.\\nI was late. Again. I was late. Again. I was late. Again. I was late. Again. I was late. Again. I was late']\n",
      "query: They were trapped. No escape\n",
      ", time: 3.072643995285034\n",
      "['They were trapped. No escape\\nTheir only hope was to fight\\nTheir only chance was to run\\nTheir only chance was to hide\\nTheir only chance was to pray\\nTheir only chance was to pray.']\n",
      "query: She couldn’t believe her eyes\n",
      ", time: 3.017131805419922\n",
      "['She couldn’t believe her eyes\\nShe couldn’t believe her eyes.\\nShe was looking at the sky\\nAnd she saw a shooting star.\\nShe was looking at the sky,\\nAnd she saw a shooting star.\\n']\n",
      "query: He heard a scream. He ran\n",
      ", time: 3.0394041538238525\n",
      "['He heard a scream. He ran\\nHe heard a scream. He ran to the source of the scream. He saw a man with a knife. He ran to the man with the knife. He saw a woman.']\n",
      "query: They kissed. Fireworks exploded\n",
      ", time: 3.071495532989502\n",
      "['They kissed. Fireworks exploded\\nTheir lips met.\\nTheir bodies touched.\\nTheir hearts beat.\\nTheir souls connected.\\nThey kissed.\\nFireworks exploded.\\nTheir lips met']\n",
      "query: She had a choice. A hard one\n",
      ", time: 3.01975417137146\n",
      "['She had a choice. A hard one\\nShe had a choice. A hard one.\\nShe could either stay in the house and be with her family, or she could go out and be with her friends.\\nShe chose to go out']\n",
      "query: He had always wanted to fly\n",
      ", time: 3.053399085998535\n",
      "['He had always wanted to fly\\nHe had always wanted to fly.\\nHe had always wanted to fly. He had always wanted to fly. He had always wanted to fly. He had always wanted to fly. He had always wanted']\n",
      "query: She was the best detective in town\n",
      ", time: 3.0546767711639404\n",
      "['She was the best detective in town\\nShe was the best detective in town.\\nShe was the best detective in town. She was the best detective in town. She was the best detective in town. She was the']\n",
      "query: The war was over. But not for him\n",
      ", time: 3.0297112464904785\n",
      "['The war was over. But not for him\\nThe war was over. But not for him.\\nThe war was over. But not for him. The war was over. But not for him. The war was over. But not for him']\n",
      "query: She had a gift. A dangerous one\n",
      ", time: 3.022963762283325\n",
      "['She had a gift. A dangerous one\\nShe had a gift. A dangerous one.\\nShe was a young woman, a girl really, who had a gift. A dangerous one. She could see the future. She could see the past']\n",
      "query: He didn’t expect to find love\n",
      ", time: 3.0181992053985596\n",
      "['He didn’t expect to find love\\n“I was a single dad, and I didn’t expect to find love again,” says John. “I was just trying to get through the day.”\\nJohn’s wife had died']\n",
      "query: She was lost in the woods\n",
      ", time: 3.0167946815490723\n",
      "['She was lost in the woods\\nShe was lost in the woods.\\nShe was lost in the woods. She was lost in the woods. She was lost in the woods. She was lost in the woods. She was lost in']\n",
      "query: The virus was spreading. Fast\n",
      ", time: 3.034236192703247\n",
      "['The virus was spreading. Fast\\nThe virus was spreading. Fast.\\nThe virus was spreading. Fast.\\nThe virus was spreading. Fast. The virus was spreading. Fast. The virus was spreading.']\n",
      "query: He had a mission. A secret one\n",
      ", time: 3.062077283859253\n",
      "['He had a mission. A secret one\\nHe had a mission. A secret one.\\nHe was a man of few words. He was a man of few words.\\nHe was a man of few words. He was a man of']\n",
      "query: She was a princess. But not by choice\n",
      ", time: 3.0319101810455322\n",
      "['She was a princess. But not by choice\\nBy: Jill Tucker\\nSAN FRANCISCO — The first time she met the man who would become her husband, she was 16.\\nShe was a princess']\n",
      "query: He was a thief. A master one\n",
      ", time: 3.036043167114258\n",
      "['He was a thief. A master one\\nHe was a thief. A master one. He was a thief. A master one. He was a thief. A master one. He was a thief. A master one.']\n",
      "[1.241760015487671, 1.1983413696289062, 1.1540663242340088, 1.1993374824523926, 1.246180534362793, 1.1699597835540771, 1.1699461936950684, 1.186004400253296, 1.1689057350158691, 2.9811220169067383, 3.141934394836426, 3.103708505630493, 3.046123504638672, 3.026996612548828, 3.011507749557495, 3.072643995285034, 3.017131805419922, 3.0394041538238525, 3.071495532989502, 3.01975417137146, 3.053399085998535, 3.0546767711639404, 3.0297112464904785, 3.022963762283325, 3.0181992053985596, 3.0167946815490723, 3.034236192703247, 3.062077283859253, 3.0319101810455322, 3.036043167114258]\n",
      "[40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40]\n",
      "每秒 token 数: 16.080114160535672\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# 记录每次生成的时间和 token 数量\n",
    "time_list = []\n",
    "token_list = []\n",
    "\n",
    "query_list = [\n",
    "    \"I look forward to\",\n",
    "    \"I love beijing , because\",\n",
    "]\n",
    "with open(\"./data/query.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    query_list = f.readlines()\n",
    "\n",
    "for query in query_list:\n",
    "    inputs = tokenizer(query, return_tensors=\"pt\").to(model.device)\n",
    "    start = time.time()\n",
    "    outputs = model.generate(**inputs, max_new_tokens=40)\n",
    "    end = time.time()\n",
    "    print(f\"query: {query}, time: {end - start}\")\n",
    "    print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
    "\n",
    "    time_list.append(end - start)\n",
    "    token_list.append(outputs.shape[1] - inputs.input_ids.shape[1])\n",
    "\n",
    "print(time_list)\n",
    "print(token_list)\n",
    "# 计算每秒生成的 token 数量\n",
    "print(\"每秒 token 数:\", sum(token_list) / sum(time_list))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
